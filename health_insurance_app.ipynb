{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPp7E4JrjWf1HDKO0/82GoU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gsowndarya/Insurance/blob/main/health_insurance_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions --quiet"
      ],
      "metadata": {
        "id": "UmdlExURHI5G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok pandas scikit-learn --quiet"
      ],
      "metadata": {
        "id": "AZyFYJD8akC4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok --quiet"
      ],
      "metadata": {
        "id": "UfktIG97RnKQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"ngrok config add-authtoken 2xJRPyoirSy9D8YpdP88rm6tWgH_6ocNEUgvkuU6Zyh5xn8NS\")"
      ],
      "metadata": {
        "id": "D5Ps6qxcc4zn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import LeakyReLU"
      ],
      "metadata": {
        "id": "4CaJGlUClEGl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "# Create an empty dictionary to store dataframes\n",
        "dataframes = {}\n",
        "\n",
        "# Load each file into a separate DataFrame\n",
        "for file_name in uploaded.keys():\n",
        "    df = pd.read_csv(file_name)\n",
        "    dataframes[file_name] = df\n",
        "    print(f\"Dataset '{file_name}' uploaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ZN8H2Wjp7K6c",
        "outputId": "113c1717-909f-4eb2-f1f1-ff108b0c21e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-454ab467-8983-4a14-9dab-ee5128876067\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-454ab467-8983-4a14-9dab-ee5128876067\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving claim_base_data.csv to claim_base_data (1).csv\n",
            "Saving insurance_class_reg_data.csv to insurance_class_reg_data (1).csv\n",
            "Dataset 'claim_base_data (1).csv' uploaded successfully!\n",
            "Dataset 'insurance_class_reg_data (1).csv' uploaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataframes[\"insurance_class_reg_data.csv\"]\n",
        "df_1= dataframes[\"claim_base_data.csv\"]"
      ],
      "metadata": {
        "id": "pE0VimPk9WUT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "# Create an empty dictionary to store dataframes\n",
        "dataframes = {}\n",
        "\n",
        "# Load each file into a separate DataFrame\n",
        "for file_name in uploaded.keys():\n",
        "    df = pd.read_csv(file_name)\n",
        "    dataframes[file_name] = df\n",
        "    print(f\"Dataset '{file_name}' uploaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "xpCPaKN174Fa",
        "outputId": "3ead7e9b-1cff-4064-a550-8c182d6be136"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ccdcdc7-8065-4875-a857-2408f96ebc17\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ccdcdc7-8065-4875-a857-2408f96ebc17\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cus_seg.csv to cus_seg.csv\n",
            "Saving insurance.csv to insurance.csv\n",
            "Saving reviews.csv to reviews.csv\n",
            "Dataset 'cus_seg.csv' uploaded successfully!\n",
            "Dataset 'insurance.csv' uploaded successfully!\n",
            "Dataset 'reviews.csv' uploaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_2= dataframes[\"cus_seg.csv\"]\n",
        "df_3= dataframes[\"reviews.csv\"]\n",
        "df_4= dataframes[\"insurance.csv\"]"
      ],
      "metadata": {
        "id": "JbkN5oin7Lkm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Tokenizer\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# For Lemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7znA342dG5fJ",
        "outputId": "48db2a15-355b-47c7-bfb0-a8802d18427c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# upload the data\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "3ISQn5F5k43s",
        "outputId": "43f1dbbf-d7ab-413f-82f7-daa7b7fe02a8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-07b7ea69-227f-4f50-96d7-71180eabb048\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-07b7ea69-227f-4f50-96d7-71180eabb048\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving scaler_risk_score_class.pkl to scaler_risk_score_class.pkl\n",
            "Saving risk_score_class_xgb_model.pkl to risk_score_class_xgb_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved models (risk_score_model)\n",
        "with open('risk_score_class_xgb_model.pkl', 'rb') as file:\n",
        "    xgb= pickle.load(file)\n",
        "\n",
        "with open('scaler_risk_score_class.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)"
      ],
      "metadata": {
        "id": "-FqU_hxck6vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b142f7-2b67-4c92-ec80-98d27a8584df"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3432568130.py:3: UserWarning: [20:59:01] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
            "configuration generated by an older version of XGBoost, please export the model by calling\n",
            "`Booster.save_model` from that version first, then load it back in current version. See:\n",
            "\n",
            "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
            "\n",
            "for more details about differences between saving model and serializing.\n",
            "\n",
            "  xgb= pickle.load(file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved models (claim_amount_model)\n",
        "with open('claim_amount_reg_gbr_model.pkl', 'rb') as file:\n",
        "    gbr= pickle.load(file)\n",
        "\n",
        "with open('scaler_claim_reg.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)\n",
        "\n",
        "with open(\"claim_amount_reg_alpha_value.pkl\", \"rb\") as file:\n",
        "    alpha = pickle.load(file)\n",
        "\n",
        "dl = load_model(\"claim_amount_reg_deep_model_3_improved.keras\", custom_objects={'LeakyReLU': LeakyReLU})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fG4hqraR2AU",
        "outputId": "b97a7235-4e03-490a-90f3-88d8e504f300"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 24 variables whereas the saved optimizer has 46 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved models (fraud_detection_model)\n",
        "\n",
        "# Normal Fraud Detection\n",
        "with open('fraud_claim_class_lgb_model.pkl', 'rb') as file:\n",
        "    xgb= pickle.load(file)\n",
        "\n",
        "with open('scaler_fraud_claim_class.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)\n",
        "\n",
        "\n",
        "# Anomaly Detection\n",
        "with open('fraud_class_ano_model.pkl', 'rb') as file:\n",
        "    anomaly= pickle.load(file)\n",
        "\n",
        "# Anomaly Detection with ML\n",
        "with open('fraud_class_ano_model.pkl', 'rb') as file:\n",
        "    anomaly= pickle.load(file)\n",
        "\n",
        "with open('fraud_class_cm_model.pkl', 'rb') as file:\n",
        "    cm= pickle.load(file)\n",
        "\n",
        "with open('scaler_fraud_class_cm_model.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)"
      ],
      "metadata": {
        "id": "8XGkTKP_R-JE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved models (Customer Segmentation)\n",
        "with open('cus_seg_knn_dbscan.pkl', 'rb') as file:\n",
        "    xgb= pickle.load(file)\n",
        "\n",
        "with open('cus_seg_scaler.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)"
      ],
      "metadata": {
        "id": "p9AgOr5OSFnk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved models (NLP)\n",
        "with open('nlp_model.pkl', 'rb') as file:\n",
        "    log= pickle.load(file)\n",
        "\n",
        "with open('vectorizer_nlp.pkl', 'rb') as file:\n",
        "    tfidf_vectorize= pickle.load(file)"
      ],
      "metadata": {
        "id": "JxXhuA77GyHn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.streamlit/\n",
        "\n",
        "with open(\"/root/.streamlit/config.toml\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "[theme]\n",
        "base=\"dark\"\n",
        "primaryColor=\"#ffffff\"\n",
        "backgroundColor=\"#000000\"\n",
        "secondaryBackgroundColor=\"#262730\"\n",
        "textColor=\"#ffffff\"\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "AKdnzZXOFlFL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load models and scalers\n",
        "# Risk score\n",
        "@st.cache_resource\n",
        "def load_risk_score_model():\n",
        "    with open(\"risk_score_class_xgb_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"scaler_risk_score_class.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "# Claim Amount Models\n",
        "@st.cache_resource\n",
        "def load_claim_amount_ml():\n",
        "    with open(\"claim_amount_reg_gbr_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"scaler_claim_reg.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "@st.cache_resource\n",
        "def load_claim_amount_dl():\n",
        "    with open(\"scaler_claim_reg.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    dl_model = load_model(\"claim_amount_reg_deep_model_3_improved.keras\", custom_objects={'LeakyReLU': LeakyReLU})\n",
        "    return dl_model, scaler\n",
        "\n",
        "# Fraud Detection Model\n",
        "@st.cache_resource\n",
        "def load_fraud_detection_model():\n",
        "    with open(\"fraud_claim_class_lgb_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"scaler_fraud_claim_class.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "@st.cache_resource\n",
        "def load_anomaly_detection_model():\n",
        "    with open(\"fraud_class_ano_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    return model\n",
        "\n",
        "@st.cache_resource\n",
        "def load_anomaly_ml_models():\n",
        "    # Load anomaly detection model\n",
        "    with open(\"fraud_class_ano_model.pkl\", \"rb\") as f:\n",
        "        anomaly_model = pickle.load(f)\n",
        "    # Load ML model for fraud/anomaly classification\n",
        "    with open(\"fraud_class_cm_model.pkl\", \"rb\") as f:\n",
        "        ml_model = pickle.load(f)\n",
        "    # Load scaler if ML model needs scaling\n",
        "    with open(\"scaler_fraud_class_cm_model.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return anomaly_model, ml_model, scaler\n",
        "\n",
        "# Customer Segmentation\n",
        "@st.cache_resource\n",
        "def load_customer_segmentation_model():\n",
        "    with open(\"cus_seg_knn_dbscan.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"cus_seg_scaler.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "# Sentiment Analysis\n",
        "@st.cache_resource\n",
        "def load_sentiment_analysis_model():\n",
        "    with open(\"nlp_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"vectorizer_nlp.pkl\", \"rb\") as f:\n",
        "        vector = pickle.load(f)\n",
        "    return model, vector\n",
        "\n",
        "#st.markdown(\n",
        "#    \"\"\"\n",
        "#    <style>\n",
        "#        .stApp {\n",
        "#            background-color: black;\n",
        "#            color: white;\n",
        "#        }\n",
        "#    </style>\n",
        "#    \"\"\",\n",
        "#    unsafe_allow_html=True\n",
        "#)\n",
        "\n",
        "st.title(\"üõ°Ô∏è Insurance Prediction Dashboard\")\n",
        "st.sidebar.title(\"üîç Navigation\")\n",
        "main_section = st.sidebar.radio(\n",
        "    \"Choose Section\",\n",
        "    [\"Prediction\", \"Visualization\", \"Chatbot\"]\n",
        ")\n",
        "\n",
        "# ------------\n",
        "# Prediction |\n",
        "# ------------\n",
        "\n",
        "if main_section == \"Prediction\":\n",
        "    problem = st.selectbox(\n",
        "        \"Select Prediction Task\",\n",
        "        [\"Risk Score\", \"Claim Amount\", \"Fraud Detection\", \"Customer Segmentation\", \"Sentiment Analysis\"]\n",
        "    )\n",
        "\n",
        "    # For Risk Score Prediction\n",
        "    if problem == \"Risk Score\":\n",
        "        st.header(\"Risk Score Prediction\")\n",
        "\n",
        "        # ---- Type Input Form ----\n",
        "        customer_age = st.number_input(\"Customer Age\", 18, 100, 45)\n",
        "        monthly_income = st.number_input(\"Monthly Income\", 0, 1_000_000, 57926)\n",
        "        vehicle_or_property_age = st.number_input(\"Vehicle/Property Age\", 0, 50, 6)\n",
        "        claim_history = st.number_input(\"Claim History Count\", 0, 100, 0)\n",
        "        fraudulent_claim = st.selectbox(\"Fraudulent Claim\", [\"No\", \"Yes\"])\n",
        "        premium_amount = st.number_input(\"Premium Amount\", 0, 1_000_000, 1100)\n",
        "        claim_amount = st.number_input(\"Claim Amount\", 0, 1_000_000, 719)\n",
        "        gender = st.selectbox(\"Gender\", [\"Female\", \"Male\", \"Other\"])\n",
        "        policy_type = st.selectbox(\"Policy Type\", [\"Auto\", \"Health\", \"Life\", \"Property\"])\n",
        "\n",
        "        # ---- Encoding ----\n",
        "        fraudulent_claim_enc = 1 if fraudulent_claim == \"Yes\" else 0\n",
        "        gender_encodings = {\"Female\": [1,0,0], \"Male\": [0,1,0], \"Other\": [0,0,1]}\n",
        "        policy_type_encodings = {\n",
        "            \"Auto\": [1,0,0,0], \"Health\": [0,1,0,0], \"Life\": [0,0,1,0], \"Property\": [0,0,0,1]\n",
        "        }\n",
        "\n",
        "        input_features = [\n",
        "            customer_age,\n",
        "            monthly_income,\n",
        "            vehicle_or_property_age,\n",
        "            claim_history,\n",
        "            fraudulent_claim_enc,\n",
        "            premium_amount,\n",
        "            claim_amount,\n",
        "            *gender_encodings[gender],\n",
        "            *policy_type_encodings[policy_type]\n",
        "        ]\n",
        "\n",
        "        columns = [\n",
        "            'customer_age', 'monthly_income', 'vehicle_or_property_age', 'claim_history',\n",
        "            'fraudulent_claim', 'premium_amount', 'claim_amount',\n",
        "            'gender_Female', 'gender_Male', 'gender_Other',\n",
        "            'policy_type_Auto', 'policy_type_Health', 'policy_type_Life', 'policy_type_Property'\n",
        "        ]\n",
        "\n",
        "        input_df = pd.DataFrame([input_features], columns=columns)\n",
        "\n",
        "        if st.button(\"Predict Risk Score\"):\n",
        "          model, scaler = load_risk_score_model()\n",
        "          input_scaled = scaler.transform(input_df)\n",
        "          pred = model.predict(input_scaled)[0]\n",
        "          risk_map = {0: \"LOW\", 1: \"MEDIUM\", 2: \"HIGH\"}\n",
        "          predicted_label = risk_map.get(pred, \"Unknown\")\n",
        "\n",
        "          # Color map for predictions\n",
        "          color_map = {\"LOW\": \"green\",\n",
        "                      \"MEDIUM\": \"orange\",\n",
        "                      \"HIGH\": \"red\"}\n",
        "\n",
        "          color = color_map.get(predicted_label, \"black\")\n",
        "\n",
        "          st.markdown(\n",
        "              f\"<h4>Predicted Risk Level: <span style='color:{color}'>{predicted_label}</span></h4>\",\n",
        "              unsafe_allow_html=True)\n",
        "\n",
        "    # For Claim Amount Prediction\n",
        "    elif problem == \"Claim Amount\":\n",
        "        st.header(\"Claim Amount Prediction\")\n",
        "\n",
        "        # Inputs\n",
        "        customer_age = st.number_input(\"Customer Age\", 18, 100, 45)\n",
        "        monthly_income = st.number_input(\"Monthly Income\", 0, 1_000_000, 57926)\n",
        "        vehicle_or_property_age = st.number_input(\"Vehicle/Property Age\", 0, 50, 6)\n",
        "        claim_history = st.number_input(\"Claim History Count\", 0, 100, 0)\n",
        "        fraudulent_claim = st.selectbox(\"Fraudulent Claim\", [\"No\", \"Yes\"])\n",
        "        premium_amount = st.number_input(\"Premium Amount\", 0, 1_000_000, 1100)\n",
        "        risk_score = st.number_input(\"Risk Score\", 0, 2, 1)\n",
        "\n",
        "        gender = st.selectbox(\"Gender\", [\"Female\", \"Male\", \"Other\"])\n",
        "        policy_type = st.selectbox(\"Policy Type\", [\"Auto\", \"Health\", \"Life\", \"Property\"])\n",
        "\n",
        "        # Encoding\n",
        "        fraudulent_claim_enc = 1 if fraudulent_claim == \"Yes\" else 0\n",
        "        gender_encodings = {\"Female\": [1,0,0], \"Male\": [0,1,0], \"Other\": [0,0,1]}\n",
        "        policy_type_encodings = {\n",
        "            \"Auto\": [1,0,0,0], \"Health\": [0,1,0,0], \"Life\": [0,0,1,0], \"Property\": [0,0,0,1]\n",
        "        }\n",
        "\n",
        "        input_features = [\n",
        "            customer_age,\n",
        "            monthly_income,\n",
        "            vehicle_or_property_age,\n",
        "            claim_history,\n",
        "            fraudulent_claim_enc,\n",
        "            premium_amount,\n",
        "            risk_score,\n",
        "            *gender_encodings[gender],\n",
        "            *policy_type_encodings[policy_type]\n",
        "        ]\n",
        "\n",
        "        columns = [\n",
        "            'customer_age', 'monthly_income', 'vehicle_or_property_age', 'claim_history',\n",
        "            'fraudulent_claim', 'premium_amount', 'risk_score',\n",
        "            'gender_Female', 'gender_Male', 'gender_Other',\n",
        "            'policy_type_Auto', 'policy_type_Health', 'policy_type_Life', 'policy_type_Property'\n",
        "        ]\n",
        "\n",
        "        input_df = pd.DataFrame([input_features], columns=columns)\n",
        "\n",
        "        # Model selection\n",
        "        model_type = st.radio(\"Select Model\", [\"ML\", \"DL\", \"Hybrid\"])\n",
        "        if model_type == \"Hybrid\":\n",
        "            with open(\"claim_amount_reg_alpha_value.pkl\", \"rb\") as file:\n",
        "                alpha = pickle.load(file)\n",
        "\n",
        "\n",
        "        if st.button(\"Predict Claim Amount\"):\n",
        "            if model_type == \"ML\":\n",
        "                ml_model, scaler = load_claim_amount_ml()\n",
        "                scaled_input = scaler.transform(input_df)\n",
        "                prediction = ml_model.predict(scaled_input)[0]\n",
        "\n",
        "            elif model_type == \"DL\":\n",
        "                dl_model, scaler = load_claim_amount_dl()\n",
        "                scaled_input = scaler.transform(input_df)\n",
        "                prediction = dl_model.predict(scaled_input).flatten()[0]\n",
        "\n",
        "            else:  # Hybrid\n",
        "                ml_model, scaler = load_claim_amount_ml()\n",
        "                dl_model, scaler = load_claim_amount_dl()\n",
        "\n",
        "                scaled_input = scaler.transform(input_df)\n",
        "                pred_ml = ml_model.predict(scaled_input)[0]\n",
        "                pred_dl = dl_model.predict(scaled_input).flatten()[0]\n",
        "\n",
        "                prediction = alpha * pred_ml + (1 - alpha) * pred_dl\n",
        "\n",
        "            st.success(f\"Predicted Claim Amount: {prediction:,.2f}\")\n",
        "\n",
        "    # For Fraud Detection\n",
        "    elif problem == \"Fraud Detection\":\n",
        "        st.header(\"Fraud Detection\")\n",
        "\n",
        "        sub_problem = st.radio(\"Select Sub-Category\",\n",
        "                              [\"Normal Fraud Detection\", \"Anomaly Detection\", \"Anomaly Detection with ML\"])\n",
        "\n",
        "        # Common Inputs\n",
        "        customer_age = st.number_input(\"Customer Age\", 18, 100, 45)\n",
        "        monthly_income = st.number_input(\"Monthly Income\", 0, 1_000_000, 57926)\n",
        "        vehicle_or_property_age = st.number_input(\"Vehicle/Property Age\", 0, 50, 6)\n",
        "        claim_history = st.number_input(\"Claim History Count\", 0, 100, 0)\n",
        "        premium_amount = st.number_input(\"Premium Amount\", 0, 1_000_000, 1100)\n",
        "        claim_amount = st.number_input(\"Claim Amount\", 0, 1_000_000, 719)\n",
        "        risk_score = st.number_input(\"Risk Score\", 0, 2, 1)  # 0=LOW, 1=MEDIUM, 2=HIGH\n",
        "        gender = st.selectbox(\"Gender\", [\"Female\", \"Male\", \"Other\"])\n",
        "        policy_type = st.selectbox(\"Policy Type\", [\"Auto\", \"Health\", \"Life\", \"Property\"])\n",
        "\n",
        "        gender_encodings = {\"Female\": [1,0,0], \"Male\": [0,1,0], \"Other\": [0,0,1]}\n",
        "        policy_type_encodings = {\n",
        "            \"Auto\": [1,0,0,0], \"Health\": [0,1,0,0], \"Life\": [0,0,1,0], \"Property\": [0,0,0,1]\n",
        "        }\n",
        "\n",
        "        input_features = [\n",
        "            customer_age,\n",
        "            monthly_income,\n",
        "            vehicle_or_property_age,\n",
        "            claim_history,\n",
        "            premium_amount,\n",
        "            claim_amount,\n",
        "            risk_score,\n",
        "            *gender_encodings[gender],\n",
        "            *policy_type_encodings[policy_type]\n",
        "        ]\n",
        "\n",
        "        columns = [\n",
        "            'customer_age', 'monthly_income', 'vehicle_or_property_age', 'claim_history',\n",
        "            'premium_amount', 'claim_amount', 'risk_score',\n",
        "            'gender_Female', 'gender_Male', 'gender_Other',\n",
        "            'policy_type_Auto', 'policy_type_Health', 'policy_type_Life', 'policy_type_Property'\n",
        "        ]\n",
        "\n",
        "        input_df = pd.DataFrame([input_features], columns=columns)\n",
        "\n",
        "\n",
        "        # --- Normal Fraud Detection ---\n",
        "        if sub_problem == \"Normal Fraud Detection\":\n",
        "            if st.button(\"Predict Fraud\"):\n",
        "                model, scaler = load_fraud_detection_model()\n",
        "                input_scaled = scaler.transform(input_df)\n",
        "                pred = model.predict(input_scaled)[0]\n",
        "                fraud_map = {0: \"NOT FRAUD\", 1: \"FRAUD\"}\n",
        "                predicted_label = fraud_map.get(pred, \"Unknown\")\n",
        "                color_map = {\"NOT FRAUD\": \"green\", \"FRAUD\": \"red\"}\n",
        "                color = color_map.get(predicted_label, \"black\")\n",
        "                st.markdown(\n",
        "                    f\"<h4>Prediction: <span style='color:{color}'>{predicted_label}</span></h4>\",\n",
        "                    unsafe_allow_html=True\n",
        "                )\n",
        "\n",
        "        # --- Anomaly Detection ---\n",
        "        elif sub_problem == \"Anomaly Detection\":\n",
        "            if st.button(\"Detect Anomaly\"):\n",
        "                model = load_anomaly_detection_model()\n",
        "\n",
        "                # Predict (-1 for anomaly, 1 for normal)\n",
        "                anomaly_status = model.predict(input_df)[0]\n",
        "\n",
        "                # Convert to 0/1 for mapping consistency\n",
        "                anomaly_flag = 1 if anomaly_status == -1 else 0\n",
        "                anomaly_map = {0: \"NORMAL\", 1: \"ANOMALY\"}\n",
        "                predicted_label = anomaly_map[anomaly_flag]\n",
        "\n",
        "                # Color coding\n",
        "                color_map = {\"NORMAL\": \"green\", \"ANOMALY\": \"red\"}\n",
        "                color = color_map.get(predicted_label, \"black\")\n",
        "                st.markdown(\n",
        "                    f\"<h4>Anomaly Detection Result: <span style='color:{color}'>{predicted_label}</span></h4>\",\n",
        "                    unsafe_allow_html=True\n",
        "                )\n",
        "\n",
        "        # --- Anomaly Detection with ML ---\n",
        "        elif sub_problem == \"Anomaly Detection with ML\":\n",
        "            st.subheader(\"Anomaly Detection with ML\")\n",
        "\n",
        "            if st.button(\"Run Anomaly + ML Prediction\"):\n",
        "                anomaly_model, ml_model, scaler = load_anomaly_ml_models()\n",
        "\n",
        "                # Step 1: Run anomaly detection\n",
        "                anomaly_status = anomaly_model.predict(input_df)[0]  # -1 = anomaly, 1 = normal\n",
        "                anomaly_flag = 1 if anomaly_status == -1 else 0\n",
        "\n",
        "                anomaly_map = {0: \"NORMAL\", 1: \"ANOMALY\"}\n",
        "                anomaly_label = anomaly_map[anomaly_flag]\n",
        "\n",
        "                # Step 2: Add anomaly flag to features\n",
        "                input_df['anomaly_flag'] = anomaly_flag\n",
        "\n",
        "                # Step 3: Scale for ML model\n",
        "                scaled_input = scaler.transform(input_df)\n",
        "\n",
        "                # Step 4: Predict with ML model\n",
        "                pred_label = ml_model.predict(scaled_input)[0]\n",
        "\n",
        "                result_map = {0: \"LEGITIMATE\", 1: \"FRAUD\"}\n",
        "                ml_label = result_map.get(pred_label, \"Unknown\")\n",
        "\n",
        "                # Colors\n",
        "                color_map = {\"NORMAL\": \"green\", \"ANOMALY\": \"red\"}\n",
        "                color_map_ml = {\"LEGITIMATE\": \"green\", \"FRAUD\": \"red\"}\n",
        "                #color = color_map.get(predicted_label, \"black\")\n",
        "\n",
        "                st.markdown(f\"<h4>Anomaly Detection Result: <span style='color:{color_map[anomaly_label]}'>{anomaly_label}</span></h4>\", unsafe_allow_html=True)\n",
        "                st.markdown(f\"<h4>ML Classification Result: <span style='color:{color_map_ml[ml_label]}'>{ml_label}</span></h4>\", unsafe_allow_html=True)\n",
        "\n",
        "    elif problem == \"Customer Segmentation\":\n",
        "        st.subheader(\"Customer Segmentation\")\n",
        "\n",
        "        # User inputs\n",
        "        customer_age = st.number_input(\"Customer Age\", min_value=18, max_value=100, value=30)\n",
        "        monthly_income = st.number_input(\"Monthly Income\", min_value=0, value=50000)\n",
        "        claim_history = st.number_input(\"Claim History\", min_value=0, value=0)\n",
        "        policy_upgrade = st.number_input(\"Policy Upgrade\", min_value=0, value=7)\n",
        "        num_active_policies = st.number_input(\"Number of Active Policies\", min_value=0, value=1)\n",
        "\n",
        "        if st.button(\"Predict Customer Segment\"):\n",
        "            # Load ML model + scaler\n",
        "            model, scaler = load_customer_segmentation_model()\n",
        "\n",
        "            # Create DataFrame with the same column order as training\n",
        "            input_df = pd.DataFrame([[\n",
        "                customer_age,\n",
        "                monthly_income,\n",
        "                claim_history,\n",
        "                policy_upgrade,\n",
        "                num_active_policies\n",
        "            ]], columns=[\n",
        "                'customer_age',\n",
        "                'monthly_income',\n",
        "                'claim_history',\n",
        "                'policy_upgrade',\n",
        "                'num_active_policies'\n",
        "            ])\n",
        "\n",
        "            # Scale\n",
        "            scaled_input = scaler.transform(input_df)\n",
        "\n",
        "            # Predict\n",
        "            pred_segment = model.predict(scaled_input)[0]\n",
        "\n",
        "            # Mapping\n",
        "            segment_map = {\n",
        "                0: \"Engaged Upgraders\",\n",
        "                1: \"Passive Customers\",\n",
        "                2: \"Upgrade Enthusiasts\"\n",
        "            }\n",
        "            segment_label = segment_map.get(pred_segment, \"Unknown\")\n",
        "\n",
        "            color_map = {\n",
        "                \"Engaged Upgraders\": \"green\",\n",
        "                \"Passive Customers\": \"orange\",\n",
        "                \"Upgrade Enthusiasts\": \"pink\"\n",
        "            }\n",
        "\n",
        "            st.markdown(\n",
        "                f\"<h4>Predicted Customer Segment: <span style='color:{color_map[segment_label]}'>{segment_label}</span></h4>\",\n",
        "                unsafe_allow_html=True\n",
        "            )\n",
        "\n",
        "    # For Sentiment Analysis Prediction\n",
        "    if problem == \"Sentiment Analysis\":\n",
        "        st.header(\"Sentiment Analysis Prediction\")\n",
        "\n",
        "        model, vector = load_sentiment_analysis_model()\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        def clean_text(text):\n",
        "            text = str(text).lower()\n",
        "            text = contractions.fix(text)\n",
        "            text = re.sub(' +', ' ', text)\n",
        "            text = re.sub(r'\\[.*?\\]', '', text)\n",
        "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "            text = re.sub(r'<.*?>+', '', text)\n",
        "            text = re.sub(r'\\n', '', text)\n",
        "            text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "            text = re.sub(r'[^a-z\\s]', '', text)\n",
        "            text = re.sub(r\"\\d+\", \"\", text)\n",
        "            tokens = nltk.word_tokenize(text)\n",
        "            tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "            return \" \".join(tokens)\n",
        "\n",
        "        def predict_sentiment(new_text):\n",
        "            cleaned = clean_text(new_text)\n",
        "            x_tfidf = vector.transform([cleaned])\n",
        "            pred = model.predict(x_tfidf)[0]\n",
        "            return pred\n",
        "\n",
        "        st.write(\"Enter a customer review below to analyze its sentiment:\")\n",
        "\n",
        "        user_input = st.text_area(\"‚úçÔ∏è Customer Review\")\n",
        "\n",
        "        if st.button(\"Predict Sentiment\"):\n",
        "            if user_input.strip() == \"\":\n",
        "                st.warning(\"Please enter some text.\")\n",
        "            else:\n",
        "                prediction = predict_sentiment(user_input)\n",
        "\n",
        "            # Color-coded result\n",
        "                if prediction.lower() == \"positive\":\n",
        "                    st.markdown(f\"**Predicted Sentiment:** <span style='color:green'>{prediction}</span>\", unsafe_allow_html=True)\n",
        "                elif prediction.lower() == \"neutral\":\n",
        "                    st.markdown(f\"**Predicted Sentiment:** <span style='color:blue'>{prediction}</span>\", unsafe_allow_html=True)\n",
        "                elif prediction.lower() == \"negative\":\n",
        "                    st.markdown(f\"**Predicted Sentiment:** <span style='color:red'>{prediction}</span>\", unsafe_allow_html=True)\n",
        "                else:\n",
        "                    st.write(f\"Predicted Sentiment: {prediction}\")  # fallback\n",
        "\n",
        "# ---------------\n",
        "# Visualization |\n",
        "# ---------------\n",
        "\n",
        "\n",
        "elif main_section == \"Visualization\":\n",
        "    st.header(\"üìä Visualization Dashboard\")\n",
        "\n",
        "    # Dataset selector\n",
        "    dataset_choice = st.selectbox(\n",
        "        \"Select Dataset to Visualize\",\n",
        "        [\"Customer Data\",\"Customer Claim\", \"Customer Segments\", \"Customer Reviews\"]\n",
        "    )\n",
        "\n",
        "    # -------------------------------\n",
        "    # Load dataset  - Customer Data |\n",
        "    # -------------------------------\n",
        "    if dataset_choice == \"Customer Data\":\n",
        "      df = pd.read_csv(\"insurance_class_reg_data.csv\")\n",
        "      df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"), inplace=True)\n",
        "      st.subheader(\"üìä Customer Data\")\n",
        "\n",
        "\n",
        "      viz_option = st.radio(\n",
        "          \"Choose Visualization\",\n",
        "          [\"Univariate Analysis (Numerical)\", \"Gender Distribution\", \"Policy Type Distribution\", \"Vehicle or Property Age Distribution\",\n",
        "           \"Claim History Distribution\", \"Fraudulent Claim Distribution\", \"Risk Score Distribution\",\"Location Distribution\",\n",
        "           \"Policy Upgrade Distribution\", \"Claim History by Gender\", \"Monthly Income by Gender\", \"Vehicle/Property Age by Gender\",\n",
        "           \"Policy Type by Gender\",\"Premium Amount by Gender\", \"Claim Amount by Gender\", \"Risk Score by Gender\",\n",
        "           \"Customer Age Density by Gender\",\"Customer Age by Claim History\",\"Fraudulent Claims by Gender\",\n",
        "           \"Fraudulent Claim by Risk Score\",\"Fraudulent Claim by Policy Type\", \"Claim Amount by Risk Score\",\n",
        "           \"Proportion of Risk Score by Gender and Policy Type\", \"Correlation Heatmap\",\n",
        "           \"Fraudulent Claims by Gender and Policy Type\", \"Customer Age Distribution by Policy Type\"]\n",
        "      )\n",
        "\n",
        "      # --- Univariate Analysis (Numerical) ---\n",
        "      if viz_option == \"Univariate Analysis (Numerical)\":\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        num_cols = ['customer_age', 'monthly_income', 'vehicle_or_property_age',\n",
        "                    'claim_amount', 'premium_amount', 'claim_history']\n",
        "\n",
        "        for col in num_cols:\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "            # Histogram\n",
        "            sns.histplot(df[col], bins=10, kde=True, ax=axes[0], color=\"skyblue\")\n",
        "            axes[0].set_title(f\"Histogram of {col}\")\n",
        "\n",
        "            # Boxplot\n",
        "            sns.boxplot(y=df[col], ax=axes[1], color=\"lightcoral\")\n",
        "            axes[1].set_title(f\"Boxplot of {col}\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "      # --- Gender Distribution ---\n",
        "      if viz_option == \"Gender Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='gender', palette='Set2', ax=ax)\n",
        "        ax.set_title(\"Gender Distribution\")\n",
        "        ax.set_xlabel(\"Gender\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Policy Type Distribution ---\n",
        "      if viz_option == \"Policy Type Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='policy_type', palette='Set1', ax=ax)\n",
        "        ax.set_title(\"Policy Type Distribution\")\n",
        "        ax.set_xlabel(\"Policy Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Vehicle or Property Age Distribution ---\n",
        "      if viz_option == \"Vehicle or Property Age Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='vehicle_or_property_age', palette='Set3', ax=ax)\n",
        "        ax.set_title(\"Vehicle or Property Age Distribution\")\n",
        "        ax.set_xlabel(\"Vehicle or Property Age\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "\n",
        "      # --- Claim History Distribution ---\n",
        "      if viz_option == \"Claim History Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='claim_history', palette='Set2', ax=ax)\n",
        "        ax.set_title(\"Claim History Distribution\")\n",
        "        ax.set_xlabel(\"Claim History\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claim Distribution ---\n",
        "      if viz_option == \"Fraudulent Claim Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='fraudulent_claim', ax=ax)\n",
        "        ax.set_title(\"Fraudulent Claim Distribution\")\n",
        "        ax.set_xlabel(\"Fraudulent Claim (0 = No, 1 = Yes)\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        ax.set_xticks([0, 1])\n",
        "        ax.set_xticklabels(['No Fraud', 'Fraud'])\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Risk Score Distribution ---\n",
        "      if viz_option == \"Risk Score Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        sns.countplot(data=df, x='risk_score', palette='Set2', ax=ax)\n",
        "        ax.set_title(\"Risk Score Distribution\")\n",
        "        ax.set_xlabel(\"Risk Score\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Location Distribution ---\n",
        "      if viz_option == \"Location Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        sns.countplot(data=df, x='location', ax=ax)\n",
        "        ax.set_title(\"Location Distribution\")\n",
        "        ax.set_xlabel(\"Location\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Policy Upgrade Distribution ---\n",
        "      if viz_option == \"Policy Upgrade Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        sns.countplot(data=df, x='policy_upgrade', palette='Set2', ax=ax)\n",
        "        ax.set_title(\"Policy Upgrade Distribution\")\n",
        "        ax.set_xlabel(\"Policy Upgrade\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Claim History by Gender ---\n",
        "      if viz_option == \"Claim History by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.countplot(data=df, x='claim_history', hue='gender', palette='Set2', ax=ax)\n",
        "          ax.set_title(\"Claim History by Gender\")\n",
        "          ax.set_xlabel(\"Claim History\")\n",
        "          ax.set_ylabel(\"Count\")\n",
        "          ax.legend(title='Gender')\n",
        "          plt.xticks(rotation=45)\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Monthly Income by Gender ---\n",
        "      if viz_option == \"Monthly Income by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.boxplot(x='gender', y='monthly_income', data=df, palette='pastel', ax=ax)\n",
        "          ax.set_title(\"Distribution of Monthly Income by Gender\")\n",
        "          ax.set_xlabel(\"Gender\")\n",
        "          ax.set_ylabel(\"Monthly Income\")\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Vehicle/Property Age by Gender ---\n",
        "      if viz_option == \"Vehicle/Property Age by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.kdeplot(data=df, x='vehicle_or_property_age', hue='gender', fill=True, ax=ax)\n",
        "          ax.set_title(\"Distribution of Vehicle or Property Age by Gender\")\n",
        "          ax.set_xlabel(\"Vehicle or Property Age\")\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Policy Type by Gender ---\n",
        "      if viz_option == \"Policy Type by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.countplot(data=df, x='policy_type', hue='gender', palette='Set1', ax=ax)\n",
        "          ax.set_title(\"Policy Type by Gender\")\n",
        "          ax.set_xlabel(\"Policy Type\")\n",
        "          ax.set_ylabel(\"Count\")\n",
        "          ax.legend(title='Gender')\n",
        "          plt.xticks(rotation=45)\n",
        "          plt.tight_layout()\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Premium Amount by Gender ---\n",
        "      if viz_option == \"Premium Amount by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.boxplot(data=df, x='gender', y='premium_amount', palette='Set3', ax=ax)\n",
        "          ax.set_title(\"Premium Amount by Gender\")\n",
        "          ax.set_xlabel(\"Gender\")\n",
        "          ax.set_ylabel(\"Premium Amount\")\n",
        "          plt.tight_layout()\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Claim Amount by Gender ---\n",
        "      if viz_option == \"Claim Amount by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.boxplot(data=df, x='gender', y='claim_amount', palette='pastel', ax=ax)\n",
        "          ax.set_title(\"Claim Amount by Gender\")\n",
        "          ax.set_xlabel(\"Gender\")\n",
        "          ax.set_ylabel(\"Claim Amount\")\n",
        "          plt.tight_layout()\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Risk Score by Gender ---\n",
        "      if viz_option == \"Risk Score by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.countplot(data=df, x='risk_score', hue='gender', palette='Set1', ax=ax)\n",
        "          ax.set_title(\"Gender Distribution Across Risk Scores\")\n",
        "          ax.set_xlabel(\"Risk Score\")\n",
        "          ax.set_ylabel(\"Count\")\n",
        "          ax.legend(title='Gender')\n",
        "          plt.tight_layout()\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Customer Age Density by Gender ---\n",
        "      if viz_option == \"Customer Age Density by Gender\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7, 4))\n",
        "        sns.violinplot(data=df, x='gender', y='customer_age', palette='muted', ax=ax)\n",
        "        ax.set_title(\"Customer Age Density by Gender\")\n",
        "        ax.set_xlabel(\"Gender\")\n",
        "        ax.set_ylabel(\"Customer Age\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Customer Age by Claim History ---\n",
        "      if viz_option == \"Customer Age by Claim History\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7, 4))\n",
        "        sns.boxplot(data=df, x='claim_history', y='customer_age', palette='Set1', ax=ax)\n",
        "        ax.set_title(\"Customer Age Distribution by Claim History\")\n",
        "        ax.set_xlabel(\"Claim History\")\n",
        "        ax.set_ylabel(\"Customer Age\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claims by Gender ---\n",
        "      if viz_option == \"Fraudulent Claims by Gender\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        sns.countplot(\n",
        "            data=df,\n",
        "            x='fraudulent_claim',\n",
        "            hue='gender',\n",
        "            palette='Set2',\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(\"Fraudulent Claims by Gender\")\n",
        "        ax.set_xlabel(\"Fraudulent Claim (0 = No, 1 = Yes)\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        ax.legend(title='Gender')\n",
        "        ax.set_xticks([0, 1])\n",
        "        ax.set_xticklabels(['No Fraud', 'Fraud'])\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claim by Risk Score ---\n",
        "      if viz_option == \"Fraudulent Claim by Risk Score\":\n",
        "\n",
        "        df1 = df[['risk_score','fraudulent_claim']].copy()\n",
        "        df1['fraudulent_claim'] = df1['fraudulent_claim'].astype(str)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7,4))\n",
        "        sns.countplot(data=df1, x='risk_score', hue='fraudulent_claim', ax=ax, palette=\"Set2\")\n",
        "        ax.set_title(\"Fraudulent Claim by Risk Score\")\n",
        "        ax.set_xlabel(\"Risk Score\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claim by Policy Type ---\n",
        "      if viz_option == \"Fraudulent Claim by Policy Type\":\n",
        "\n",
        "        df1 = df[['fraudulent_claim', 'policy_type']].copy()\n",
        "        df1['fraudulent_claim'] = df1['fraudulent_claim'].astype(str)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7,4))\n",
        "        sns.countplot(data=df1, x='policy_type', hue='fraudulent_claim', palette=\"coolwarm\", ax=ax)\n",
        "        ax.set_title(\"Fraudulent Claim by Policy Type\")\n",
        "        ax.set_xlabel(\"Policy Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Claim Amount by Risk Score ---\n",
        "      if viz_option == \"Claim Amount by Risk Score\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7,4))\n",
        "        sns.boxplot(data=df, x='risk_score', y='claim_amount', palette=\"Set1\", ax=ax)\n",
        "        ax.set_title(\"Claim Amount by Risk Score\")\n",
        "        ax.set_xlabel(\"Risk Score\")\n",
        "        ax.set_ylabel(\"Claim Amount\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Proportion of Risk Score by Gender and Policy Type ---\n",
        "      if viz_option == \"Proportion of Risk Score by Gender and Policy Type\":\n",
        "\n",
        "        grouped = df.groupby(['gender', 'policy_type', 'risk_score']).size().reset_index(name='count')\n",
        "        pivot_df = grouped.pivot_table(\n",
        "            index=['gender', 'policy_type'],\n",
        "            columns='risk_score',\n",
        "            values='count',\n",
        "            fill_value=0\n",
        "        )\n",
        "        pivot_df_percent = pivot_df.div(pivot_df.sum(axis=1), axis=0)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10,6))\n",
        "        pivot_df_percent.plot(\n",
        "            kind='bar',\n",
        "            stacked=True,\n",
        "            ax=ax,\n",
        "            colormap='coolwarm'\n",
        "        )\n",
        "        ax.set_title(\"Proportion of Risk Score by Gender and Policy Type\")\n",
        "        ax.set_ylabel(\"Proportion\")\n",
        "        ax.legend(title=\"Risk Score\", labels=[\"Low\", \"Medium\", \"High\"])\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Correlation Heatmap --\n",
        "      if viz_option == \"Correlation Heatmap\":\n",
        "\n",
        "        numerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        sns.heatmap(\n",
        "            df[numerical_cols].corr(),\n",
        "            annot=True,\n",
        "            fmt=\".0%\",\n",
        "            cmap=\"YlGnBu\",\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(\"Correlation Heatmap of Numerical Features\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claims by Gender and Policy Type ---\n",
        "      if viz_option == \"Fraudulent Claims by Gender and Policy Type\":\n",
        "\n",
        "        df1 = df[['gender', 'policy_type', 'fraudulent_claim']].copy()\n",
        "        df1['fraudulent_claim'] = df1['fraudulent_claim'].astype(str)\n",
        "        df1['gender_policy'] = df1['gender'] + \" - \" + df1['policy_type']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        sns.countplot(\n",
        "            data=df1,\n",
        "            x='gender_policy',\n",
        "            hue='fraudulent_claim',\n",
        "            palette='coolwarm',\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(\"Fraudulent Claims by Gender and Policy Type\")\n",
        "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "        ax.set_xlabel(\"Gender - Policy Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        ax.legend(title=\"Fraudulent Claim\", labels=[\"Not Fraud\", \"Fraud\"])\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Customer Age Distribution by Policy Type ---\n",
        "      if viz_option == \"Customer Age Distribution by Policy Type\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        sns.kdeplot(\n",
        "            data=df,\n",
        "            x='customer_age',\n",
        "            hue='policy_type',\n",
        "            fill=True,\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(\"Customer Age Distribution by Policy Type\")\n",
        "        ax.set_xlabel(\"Customer Age\")\n",
        "        ax.set_ylabel(\"Density\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Load dataset - Customer Claim |\n",
        "    # -------------------------------\n",
        "\n",
        "    if dataset_choice == \"Customer Claim\":\n",
        "      df = pd.read_csv(\"claim_base_data.csv\")\n",
        "      df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"), inplace=True)\n",
        "      st.subheader(\"üìä Customer Claim Data\")\n",
        "\n",
        "\n",
        "      # Convert claim_date to datetime and extract Year + Month\n",
        "      df['claim_date'] = pd.to_datetime(df['claim_date'])\n",
        "      df['Year'] = df['claim_date'].dt.year\n",
        "      df['Month'] = df['claim_date'].dt.to_period('M')\n",
        "\n",
        "      # Let user pick a Year\n",
        "      years = sorted(df['Year'].unique())\n",
        "      selected_year = st.selectbox(\"Select Year\", years)\n",
        "\n",
        "      # Filter by selected year\n",
        "      df_year = df[df['Year'] == selected_year]\n",
        "\n",
        "      # Group by month\n",
        "      monthly_counts = df_year.groupby('Month').size()\n",
        "\n",
        "      # Plot\n",
        "      fig, ax = plt.subplots(figsize=(12, 4))\n",
        "      monthly_counts.plot(kind='bar', ax=ax, color=\"skyblue\")\n",
        "\n",
        "      ax.set_title(f\"Monthly Claim Counts ({selected_year})\")\n",
        "      ax.set_xlabel(\"Month\")\n",
        "      ax.set_ylabel(\"Number of Claims\")\n",
        "\n",
        "      plt.xticks(rotation=45)\n",
        "      st.pyplot(fig)\n",
        "\n",
        "    # ----------------------------------\n",
        "    # Load dataset - Customer Segments |\n",
        "    # ----------------------------------\n",
        "\n",
        "    if dataset_choice == \"Customer Segments\":\n",
        "      df = pd.read_csv(\"cus_seg.csv\")\n",
        "      df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"), inplace=True)\n",
        "      st.subheader(\"üìä Customer Segments Data\")\n",
        "\n",
        "      viz_option = st.radio(\n",
        "          \"Choose Visualization\",\n",
        "          [\"DBSCAN Cluster Distribution\", \"Monthly Income Distribution by DBSCAN Cluster\",\n",
        "           \"Claim History Distribution Across DBSCAN Clusters\", \"Customer Segments in 3D (PCA + DBSCAN)\"]\n",
        "          )\n",
        "\n",
        "\n",
        "      if viz_option == \"DBSCAN Cluster Distribution\":\n",
        "\n",
        "        # Plot DBSCAN cluster distribution\n",
        "        fig, ax = plt.subplots(figsize=(7,4))\n",
        "        df['dbscan_cluster'].value_counts().sort_index().plot(\n",
        "            kind='bar',\n",
        "            color=['lightcoral', 'orange', 'pink', 'skyblue'],  # custom colors\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        ax.set_title(\"Cluster Distribution\")\n",
        "        ax.set_xlabel(\"DBSCAN Cluster\")\n",
        "        ax.set_ylabel(\"Number of Customers\")\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Monthly Income Distribution by DBSCAN Cluster\":\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        # KDE plot\n",
        "        sns.kdeplot(\n",
        "          data=df,\n",
        "          x='monthly_income',\n",
        "          hue='dbscan_cluster',\n",
        "          common_norm=False,\n",
        "          ax=ax,\n",
        "          fill=True,   # Optional: fills under the curve for clarity\n",
        "          alpha=0.5    # Optional: makes it slightly transparent\n",
        "        )\n",
        "\n",
        "        ax.set_title(\"Distribution of Monthly Income by DBSCAN Cluster\", fontsize=14)\n",
        "        ax.set_xlabel(\"Monthly Income\")\n",
        "        ax.set_ylabel(\"Density\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Display in Streamlit\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Claim History Distribution Across DBSCAN Clusters\":\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        # Boxplot\n",
        "        sns.boxplot(\n",
        "            data=df,\n",
        "            x='dbscan_cluster',\n",
        "            y='claim_history',\n",
        "            ax=ax,\n",
        "            palette=\"Set2\"   # optional for colors\n",
        "        )\n",
        "\n",
        "        ax.set_title(\"Claim History Distribution Across DBSCAN Clusters\", fontsize=14)\n",
        "        ax.set_xlabel(\"DBSCAN Cluster\")\n",
        "        ax.set_ylabel(\"Claim History\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Display in Streamlit\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Customer Segments in 3D (PCA + DBSCAN)\":\n",
        "\n",
        "        fig = px.scatter_3d(\n",
        "            df,\n",
        "            x='pc1',\n",
        "            y='pc2',\n",
        "            z='pc3',\n",
        "            color='dbscan_cluster',  # Cluster label\n",
        "            title='Customer Segments in 3D (PCA + DBSCAN)',\n",
        "            labels={\n",
        "                'PC1': 'Principal Component 1',\n",
        "                'PC2': 'Principal Component 2',\n",
        "                'PC3': 'Principal Component 3'\n",
        "            },\n",
        "            opacity=0.8\n",
        "        )\n",
        "\n",
        "        # Update marker size\n",
        "        fig.update_traces(marker=dict(size=5))\n",
        "\n",
        "        # Layout settings\n",
        "        fig.update_layout(\n",
        "            template='plotly_white',\n",
        "            legend_title=\"Cluster\"\n",
        "        )\n",
        "\n",
        "        # Show in Streamlit\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "\n",
        "    # ---------------------------------\n",
        "    # Load dataset - Customer Reviews |\n",
        "    # ---------------------------------\n",
        "\n",
        "\n",
        "    if dataset_choice == \"Customer Reviews\":\n",
        "      df = pd.read_csv(\"reviews.csv\")\n",
        "      df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"), inplace=True)\n",
        "      st.subheader(\"üìä Customer Reviews Data\")\n",
        "\n",
        "      viz_option = st.radio(\n",
        "          \"Choose Visualization\",\n",
        "          [\"Sentiment Distribution\", \"Rating Distribution\", \"Rating Distribution by Service Type Status\", \"WordClouds by Sentiment\"]\n",
        "          )\n",
        "\n",
        "      if viz_option == \"Sentiment Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6,4))\n",
        "        sns.countplot(data=df, x=\"sentiment_label\", palette=\"Set2\", ax=ax)\n",
        "        ax.set_title(\"Sentiment Distribution\")\n",
        "        ax.set_xlabel(\"Sentiment Label\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Rating Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        sns.countplot(data=df, x='rating', palette='coolwarm', ax=ax)\n",
        "\n",
        "        ax.set_title('Rating Distribution')\n",
        "        ax.set_xlabel('Rating')\n",
        "        ax.set_ylabel('Count')\n",
        "\n",
        "        # Add counts on top of bars\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            ax.text(p.get_x() + p.get_width()/2., height + 1, int(height), ha=\"center\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Rating Distribution by Service Type Status\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        sns.countplot(data=df, x='rating', hue='service_type', ax=ax)\n",
        "\n",
        "        ax.set_title('Rating Distribution by Service Type Status')\n",
        "        ax.set_xlabel('Rating')\n",
        "        ax.set_ylabel('Count')\n",
        "\n",
        "        # Add counts on top of bars\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            if height > 0:\n",
        "                ax.text(\n",
        "                    p.get_x() + p.get_width()/2.,\n",
        "                    height + 1,\n",
        "                    int(height),\n",
        "                    ha=\"center\",\n",
        "                    fontsize=9\n",
        "                )\n",
        "\n",
        "        ax.legend(title='Service Type')\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "\n",
        "      def plot_wordcloud(sentiment, color=\"viridis\"):\n",
        "          text = \" \".join(df[df[\"sentiment_label\"] == sentiment][\"clean_text\"])\n",
        "\n",
        "          if not text.strip():  # If no text available\n",
        "              st.warning(f\"No reviews found for {sentiment} sentiment.\")\n",
        "              return\n",
        "\n",
        "          wc = WordCloud(\n",
        "              width=800,\n",
        "              height=400,\n",
        "              background_color=\"white\",\n",
        "              colormap=color,\n",
        "              max_words=200\n",
        "          ).generate(text)\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(10, 5))\n",
        "          ax.imshow(wc, interpolation=\"bilinear\")\n",
        "          ax.axis(\"off\")\n",
        "          ax.set_title(f\"WordCloud for {sentiment} Reviews\", fontsize=16)\n",
        "\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # Example usage inside your Streamlit viz section\n",
        "\n",
        "      if viz_option == \"WordClouds by Sentiment\":\n",
        "          st.subheader(\"WordClouds by Sentiment\")\n",
        "          plot_wordcloud(\"Positive\", color=\"Greens\")\n",
        "          plot_wordcloud(\"Neutral\", color=\"Blues\")\n",
        "          plot_wordcloud(\"Negative\", color=\"Reds\")\n",
        "\n",
        "# ---------\n",
        "# Chatbot |\n",
        "# ---------\n",
        "\n",
        "if main_section == \"Chatbot\":\n",
        "    # ================== Context Memory ==================\n",
        "    memory = {\"last_customer\": None, \"last_policy\": None, \"last_claim\": None}\n",
        "\n",
        "    # Load your dataframe (replace with your actual dataset)\n",
        "    df = pd.read_csv(\"insurance.csv\")\n",
        "    df = df.fillna({\n",
        "    \"claim_amount\": -1,\n",
        "    \"review_text\": \"\",\n",
        "    \"sentiment_label\": \"Not available\",\n",
        "    \"claim_id\": \"Not available\",\n",
        "    \"claim_date\": \"Not available\",\n",
        "    \"review_id\": \"Not available\",\n",
        "    \"rating\": 0,\n",
        "    \"service_type\": \"Unknown\"\n",
        "})\n",
        "\n",
        "    # ================== Helper Functions ==================\n",
        "    def get_customer_info(customer_id):\n",
        "        customer_data = df[df[\"customer_id\"].astype(str).str.strip() == str(customer_id).strip()]\n",
        "        if customer_data.empty:\n",
        "            return \"‚ö†Ô∏è Customer not found.\"\n",
        "\n",
        "        memory[\"last_customer\"] = customer_id\n",
        "        row = customer_data.iloc[0]\n",
        "\n",
        "        details = f\"\"\"\n",
        "    üë§ Customer ID: {row['customer_id']}\n",
        "    üéÇ Age: {row['customer_age']}\n",
        "    üöª Gender: {row['gender']}\n",
        "    üí∞ Monthly Income: {row['monthly_income']}\n",
        "    üìä Risk Score: {row['risk_score']}\n",
        "    üìç Location: {row['location']}\n",
        "    \"\"\"\n",
        "\n",
        "        policies = customer_data[[\"policy_id\", \"policy_type\", \"premium_amount\"]].drop_duplicates()\n",
        "        if not policies.empty:\n",
        "            policy_info = \"\\n\".join([f\"üìë {p.policy_id} | {p.policy_type} | üí∞ {p.premium_amount}\" for p in policies.itertuples()])\n",
        "            details += \"\\n---\\nüìå Policies:\\n\" + policy_info\n",
        "\n",
        "        claims = customer_data[[\"claim_id\", \"claim_date\", \"claim_amount\", \"fraudulent_claim\"]].dropna().drop_duplicates()\n",
        "        if not claims.empty:\n",
        "            claim_info = \"\\n\".join([f\"üìÑ {c.claim_id} | üìÖ {c.claim_date} | üí∞ {c.claim_amount} | üõë Fraud: {c.fraudulent_claim}\" for c in claims.itertuples()])\n",
        "            details += \"\\n---\\nüìå Claims:\\n\" + claim_info\n",
        "\n",
        "        reviews = customer_data[customer_data[\"review_text\"].str.strip() != \"\"]\n",
        "        if not reviews.empty:\n",
        "            review_info = \"\\n\\n\".join([\n",
        "                f\"üì¢ Review: {r.review_text}\\n\"\n",
        "                f\"üí¨ Sentiment: {'üü¢ Positive' if r.sentiment_label.lower()=='positive' else 'üî¥ Negative' if r.sentiment_label.lower()=='negative' else 'üü° Neutral' if r.sentiment_label.lower()=='neutral' else '‚ö™ Not available'}\\n\"\n",
        "                f\"‚≠ê Rating: {r.rating}\"\n",
        "                for r in reviews.itertuples()\n",
        "            ])\n",
        "\n",
        "            avg_rating = reviews[\"rating\"].mean()\n",
        "            sentiment_counts = reviews[\"sentiment_label\"].str.lower().value_counts()\n",
        "            if not sentiment_counts.empty:\n",
        "                majority_sentiment = sentiment_counts.idxmax()\n",
        "                if majority_sentiment == \"positive\":\n",
        "                    sentiment_summary = \"üü¢ Mostly Positive\"\n",
        "                elif majority_sentiment == \"negative\":\n",
        "                    sentiment_summary = \"üî¥ Mostly Negative\"\n",
        "                elif majority_sentiment == \"neutral\":\n",
        "                    sentiment_summary = \"üü° Mostly Neutral\"\n",
        "                else:\n",
        "                    sentiment_summary = \"‚ö™ Mixed/Unknown\"\n",
        "            else:\n",
        "                sentiment_summary = \"‚ö™ No sentiment data\"\n",
        "\n",
        "            details += \"\\n---\\nüìù Customer Reviews:\\n\" + review_info\n",
        "            details += f\"\\n\\nüìä Summary ‚Üí ‚≠ê Average Rating: {avg_rating:.1f} | {sentiment_summary}\"\n",
        "\n",
        "        return details\n",
        "\n",
        "\n",
        "    def get_policy_info(policy_id, lang=\"en\"):\n",
        "        row = df[df[\"policy_id\"].astype(str).str.strip() == str(policy_id).strip()]\n",
        "        if row.empty:\n",
        "            return \"‚ö†Ô∏è Policy not found.\"\n",
        "        row = row.iloc[0]\n",
        "\n",
        "        memory[\"last_policy\"] = row[\"policy_id\"]\n",
        "        memory[\"last_customer\"] = row[\"customer_id\"]\n",
        "\n",
        "        policy_text = row.get(f\"policy_text_{lang}\", row[\"policy_text_en\"])\n",
        "        details = f\"\"\"\n",
        "    üìë Policy ID: {row['policy_id']}\n",
        "    üë§ Customer ID: {row['customer_id']}\n",
        "    üíº Policy Type: {row['policy_type']}\n",
        "    üí∞ Premium: {row['premium_amount']}\n",
        "    üåê Info: {policy_text}\n",
        "    \"\"\"\n",
        "\n",
        "        customer_policies = df[df[\"customer_id\"] == row[\"customer_id\"]][[\"policy_id\", \"policy_type\", \"premium_amount\"]].drop_duplicates()\n",
        "        other_policies = customer_policies[customer_policies[\"policy_id\"] != row[\"policy_id\"]]\n",
        "        if not other_policies.empty:\n",
        "            policy_list = \"\\n\".join([f\"üìë {p.policy_id} | {p.policy_type} | üí∞ {p.premium_amount}\" for p in other_policies.itertuples()])\n",
        "            details += \"\\n---\\nüìå Other Policies of same customer:\\n\" + policy_list\n",
        "\n",
        "        return details\n",
        "\n",
        "\n",
        "    def get_claim_info(claim_id):\n",
        "        row = df[df[\"claim_id\"].astype(str).str.strip() == str(claim_id).strip()]\n",
        "        if row.empty:\n",
        "            return \"‚ö†Ô∏è Claim not found.\"\n",
        "        row = row.iloc[0]\n",
        "\n",
        "        memory[\"last_claim\"] = row[\"claim_id\"]\n",
        "        memory[\"last_customer\"] = row[\"customer_id\"]\n",
        "\n",
        "        details = f\"\"\"\n",
        "    üìÑ Claim ID: {row['claim_id']}\n",
        "    üìÖ Claim Date: {row['claim_date']}\n",
        "    üë§ Customer ID: {row['customer_id']}\n",
        "    üí∞ Claim Amount: {row['claim_amount']}\n",
        "    üõë Fraudulent: {row['fraudulent_claim']}\n",
        "    \"\"\"\n",
        "\n",
        "        customer_claims = df[df[\"customer_id\"] == row[\"customer_id\"]][[\"claim_id\", \"claim_date\", \"claim_amount\", \"fraudulent_claim\"]].dropna().drop_duplicates()\n",
        "        other_claims = customer_claims[customer_claims[\"claim_id\"] != row[\"claim_id\"]]\n",
        "        if not other_claims.empty:\n",
        "            claim_list = \"\\n\".join([f\"üìÑ {c.claim_id} | üìÖ {c.claim_date} | üí∞ {c.claim_amount} | üõë Fraud: {c.fraudulent_claim}\" for c in other_claims.itertuples()])\n",
        "            details += \"\\n---\\nüìå Other Claims of same customer:\\n\" + claim_list\n",
        "\n",
        "        return details\n",
        "\n",
        "\n",
        "    def get_reviews(identifier):\n",
        "        if identifier.startswith(\"R\"):\n",
        "            reviews = df[df[\"review_id\"].astype(str).str.strip() == str(identifier).strip()]\n",
        "        else:\n",
        "            reviews = df[df[\"customer_id\"].astype(str).str.strip() == str(identifier).strip()]\n",
        "        reviews = reviews[reviews[\"review_text\"].str.strip() != \"\"]\n",
        "        if reviews.empty:\n",
        "            return \"‚ö†Ô∏è No reviews found.\"\n",
        "\n",
        "        msgs = []\n",
        "        for _, row in reviews.iterrows():\n",
        "            sentiment = row['sentiment_label'].strip().lower()\n",
        "            if sentiment == \"positive\":\n",
        "                sentiment_icon = \"üü¢ Positive\"\n",
        "            elif sentiment == \"negative\":\n",
        "                sentiment_icon = \"üî¥ Negative\"\n",
        "            elif sentiment == \"neutral\":\n",
        "                sentiment_icon = \"üü° Neutral\"\n",
        "            else:\n",
        "                sentiment_icon = \"‚ö™ Not available\"\n",
        "\n",
        "            msgs.append(\n",
        "                f\"üì¢ Review: {row['review_text']}\\n\"\n",
        "                f\"üí¨ Sentiment: {sentiment_icon}\\n\"\n",
        "                f\"‚≠ê Rating: {row['rating']}\"\n",
        "            )\n",
        "        return \"\\n\\n\".join(msgs)\n",
        "\n",
        "\n",
        "    def chatbot(query, lang=\"en\"):\n",
        "        query = query.strip()\n",
        "\n",
        "        if query.startswith(\"CUST\"):\n",
        "            return get_customer_info(query)\n",
        "\n",
        "        elif query.startswith(\"POL\"):\n",
        "            return get_policy_info(query, lang)\n",
        "\n",
        "        elif query.startswith(\"CLM\"):\n",
        "            return get_claim_info(query)\n",
        "\n",
        "        elif query.startswith(\"R\"):\n",
        "            return get_reviews(query)\n",
        "\n",
        "        elif query.lower() in [\"show last customer\", \"customer\"]:\n",
        "            if memory[\"last_customer\"]:\n",
        "                return get_customer_info(memory[\"last_customer\"])\n",
        "            return \"‚ö†Ô∏è No customer in memory.\"\n",
        "\n",
        "        elif query.lower() in [\"show last policy\", \"policy\"]:\n",
        "            if memory[\"last_policy\"]:\n",
        "                return get_policy_info(memory[\"last_policy\"], lang)\n",
        "            return \"‚ö†Ô∏è No policy in memory.\"\n",
        "\n",
        "        elif query.lower() in [\"show last claim\", \"claim\"]:\n",
        "            if memory[\"last_claim\"]:\n",
        "                return get_claim_info(memory[\"last_claim\"])\n",
        "            return \"‚ö†Ô∏è No claim in memory.\"\n",
        "\n",
        "        elif query.lower() in [\"reviews\", \"show reviews\"]:\n",
        "            if memory[\"last_customer\"]:\n",
        "                return get_reviews(memory[\"last_customer\"])\n",
        "            return \"‚ö†Ô∏è No customer in memory to fetch reviews.\"\n",
        "\n",
        "        else:\n",
        "            return \"ü§ñ Please provide a valid ID (CUST..., POL..., CLM..., R...) or ask contextually.\"\n",
        "\n",
        "\n",
        "    # ================== Streamlit UI ==================\n",
        "    st.title(\"üí¨ Insurance Chatbot (Customer, Policy, Claim, Reviews)\")\n",
        "    st.markdown(\"Ask with **CUST, POL, CLM, R** IDs. ‚úÖ\")\n",
        "\n",
        "    lang = st.selectbox(\"üåê Language\", [\"en\", \"fr\", \"hi\", \"es\"], index=0)\n",
        "    query = st.text_input(\"Enter Query or ID\", placeholder=\"e.g., CUST100000, POL100000, CLM100000, R00001\")\n",
        "\n",
        "    if st.button(\"Ask\"):\n",
        "        if query.strip():\n",
        "            reply = chatbot(query, lang)\n",
        "            st.text_area(\"ü§ñ Bot Reply\", reply, height=400)\n",
        "        else:\n",
        "            st.warning(\"Please enter a query.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofEd9qZtmUBg",
        "outputId": "4a71c8d5-b0bb-49a0-8b12-b08f8463aa9e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and install the latest ngrok\n",
        "!wget -qO ngrok.zip https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -o ngrok.zip -d /usr/local/bin && chmod +x /usr/local/bin/ngrok\n",
        "\n",
        "# Add your ngrok authentication token (replace below with your token)\n",
        "!ngrok config add-authtoken 2xJRPyoirSy9D8YpdP88rm6tWgH_6ocNEUgvkuU6Zyh5xn8NS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnSP-ijlhHn_",
        "outputId": "a9a6f09d-a617-441b-c184-f36a9a9e1f63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok.zip\n",
            "  inflating: /usr/local/bin/ngrok    \n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py --server.headless true\")\n",
        "\n",
        "# Start Streamlit app in a background thread\n",
        "threading.Thread(target=run_streamlit).start()\n",
        "time.sleep(5)  # wait for Streamlit to start\n",
        "\n",
        "# Start ngrok tunnel in background\n",
        "os.system(\"ngrok http 8501 &\")\n",
        "time.sleep(5)  # wait for ngrok to start\n",
        "\n",
        "# Get the public URL\n",
        "try:\n",
        "    tunnels = requests.get(\"http://localhost:4040/api/tunnels\").json()\n",
        "    public_url = tunnels['tunnels'][0]['public_url']\n",
        "    print(\"üåê Your Streamlit app is live here:\")\n",
        "    print(public_url)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error getting ngrok URL:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kWjDDCMfw5X",
        "outputId": "e55ca669-8f15-4056-fa72-f190ae46b4a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Your Streamlit app is live here:\n",
            "https://3f29bb52301b.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "V2TkWkODWx8K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2xJRPyoirSy9D8YpdP88rm6tWgH_6ocNEUgvkuU6Zyh5xn8NS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CksJI7CjApTJ",
        "outputId": "9ff1e0db-e990-49fc-e2ff-0deb76ed7820"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading, os\n",
        "\n",
        "# Kill any previous tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a tunnel on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üåç Public URL:\", public_url)\n",
        "\n",
        "# Run Streamlit in a background thread\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py --server.port 8501 --server.headless true\")\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN_Tn-_GWx5l",
        "outputId": "9121e01e-0c42-4212-a056-08946e39114d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåç Public URL: NgrokTunnel: \"https://c652ecf966e2.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hms3q8fPWx-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "woGPS9fPXg7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7U6tZFDXg_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gX3JuKsJX5jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uiiI30WfX5mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PntcQQoTX5pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GK8zmz2pX5rn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}