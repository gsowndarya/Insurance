{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gsowndarya/Insurance/blob/main/health_insurance_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmdlExURHI5G",
        "outputId": "0e935b4b-eb32-4f8b-affa-8d51604efa1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/345.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/114.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install contractions --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wi46hHoStPqp"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AZyFYJD8akC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7378a00-ea28-492c-936a-d30f8652629b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pyngrok pandas scikit-learn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VCfTrTuU9aQh"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.19.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UfktIG97RnKQ"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzlI7-4ARAX6",
        "outputId": "215d59ee-4e38-449f-db0f-83e10a49f9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community faiss-cpu sentence-transformers transformers deep-translator --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UWc86ryxRC4A"
      },
      "outputs": [],
      "source": [
        "!pip -q install sentence-transformers faiss-cpu transformers accelerate pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Ps6qxcc4zn",
        "outputId": "0167bb60-a1ad-4533-8b3c-ace891680cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2xJRPyoirSy9D8YpdP88rm6tWgH_6ocNEUgvkuU6Zyh5xn8NS\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CaJGlUClEGl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "import faiss\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ZN8H2Wjp7K6c",
        "outputId": "d5c9b5fe-b0ef-416a-9c63-9ba92768bf52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-57481a99-064f-4d70-a329-74514a69d579\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-57481a99-064f-4d70-a329-74514a69d579\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving claim_base_data.csv to claim_base_data.csv\n",
            "Saving insurance_class_reg_data.csv to insurance_class_reg_data.csv\n",
            "Dataset 'claim_base_data.csv' uploaded successfully!\n",
            "Dataset 'insurance_class_reg_data.csv' uploaded successfully!\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "# Create an empty dictionary to store dataframes\n",
        "dataframes = {}\n",
        "\n",
        "# Load each file into a separate DataFrame\n",
        "for file_name in uploaded.keys():\n",
        "    df = pd.read_csv(file_name)\n",
        "    dataframes[file_name] = df\n",
        "    print(f\"Dataset '{file_name}' uploaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pE0VimPk9WUT"
      },
      "outputs": [],
      "source": [
        "df_0 = pd.read_csv(\"insurance_class_reg_data.csv\")\n",
        "df_1 = pd.read_csv(\"claim_base_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "xpCPaKN174Fa",
        "outputId": "0460ea6a-98d6-4495-aee7-275428ca9146"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-252d38ab-707c-44b1-9750-8bc817a77952\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-252d38ab-707c-44b1-9750-8bc817a77952\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cus_seg.csv to cus_seg.csv\n",
            "Saving insurance.csv to insurance.csv\n",
            "Saving reviews.csv to reviews.csv\n",
            "Dataset 'cus_seg.csv' uploaded successfully!\n",
            "Dataset 'insurance.csv' uploaded successfully!\n",
            "Dataset 'reviews.csv' uploaded successfully!\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "# Create an empty dictionary to store dataframes\n",
        "dataframes = {}\n",
        "\n",
        "# Load each file into a separate DataFrame\n",
        "for file_name in uploaded.keys():\n",
        "    df = pd.read_csv(file_name)\n",
        "    dataframes[file_name] = df\n",
        "    print(f\"Dataset '{file_name}' uploaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JbkN5oin7Lkm"
      },
      "outputs": [],
      "source": [
        "df_2 = pd.read_csv(\"cus_seg.csv\")\n",
        "df_3 = pd.read_csv(\"reviews.csv\")\n",
        "df_4 = pd.read_csv(\"insurance.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "94JrTL8vRTfg"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"insurance.csv\")\n",
        "df = df.fillna(\"\")\n",
        "df['premium_amount'] = pd.to_numeric(df['premium_amount'], errors='coerce')\n",
        "df['claim_amount'] = pd.to_numeric(df['claim_amount'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7znA342dG5fJ",
        "outputId": "cc32c7a3-0e52-44af-8b2f-834f74308ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Tokenizer\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# For Lemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "3ISQn5F5k43s",
        "outputId": "57f8a9ae-39ae-4dae-d018-6b31df9b36a9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-70f13720-35ad-48cf-b8e9-3b57914a160d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-70f13720-35ad-48cf-b8e9-3b57914a160d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving scaler_risk_score_class.pkl to scaler_risk_score_class.pkl\n",
            "Saving risk_score_class_xgb_model.pkl to risk_score_class_xgb_model.pkl\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "# upload the data\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FqU_hxck6vl",
        "outputId": "2a28e4ee-1fb7-433f-d1d4-26da0f7ca894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3432568130.py:3: UserWarning: [06:23:47] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
            "configuration generated by an older version of XGBoost, please export the model by calling\n",
            "`Booster.save_model` from that version first, then load it back in current version. See:\n",
            "\n",
            "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
            "\n",
            "for more details about differences between saving model and serializing.\n",
            "\n",
            "  xgb= pickle.load(file)\n"
          ]
        }
      ],
      "source": [
        "# Load saved models (risk_score_model)\n",
        "with open('risk_score_class_xgb_model.pkl', 'rb') as file:\n",
        "    xgb= pickle.load(file)\n",
        "\n",
        "with open('scaler_risk_score_class.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9fG4hqraR2AU"
      },
      "outputs": [],
      "source": [
        "# Load saved models (claim_amount_model)\n",
        "with open('claim_amount_reg_gbr_model.pkl', 'rb') as file:\n",
        "    gbr= pickle.load(file)\n",
        "\n",
        "with open('scaler_claim_reg.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8XGkTKP_R-JE"
      },
      "outputs": [],
      "source": [
        "# Load saved models (fraud_detection_model)\n",
        "\n",
        "# Normal Fraud Detection\n",
        "with open('fraud_claim_class_lgb_model.pkl', 'rb') as file:\n",
        "    xgb= pickle.load(file)\n",
        "\n",
        "with open('scaler_fraud_claim_class.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)\n",
        "\n",
        "\n",
        "# Anomaly Detection\n",
        "with open('fraud_class_ano_model.pkl', 'rb') as file:\n",
        "    anomaly= pickle.load(file)\n",
        "\n",
        "# Anomaly Detection with ML\n",
        "with open('fraud_class_ano_model.pkl', 'rb') as file:\n",
        "    anomaly= pickle.load(file)\n",
        "\n",
        "with open('fraud_class_cm_model.pkl', 'rb') as file:\n",
        "    cm= pickle.load(file)\n",
        "\n",
        "with open('scaler_fraud_class_cm_model.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "p9AgOr5OSFnk"
      },
      "outputs": [],
      "source": [
        "# Load saved models (Customer Segmentation)\n",
        "with open('cus_seg_knn_dbscan.pkl', 'rb') as file:\n",
        "    xgb= pickle.load(file)\n",
        "\n",
        "with open('cus_seg_scaler.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JxXhuA77GyHn"
      },
      "outputs": [],
      "source": [
        "# Load saved models (NLP)\n",
        "with open('nlp_model.pkl', 'rb') as file:\n",
        "    log= pickle.load(file)\n",
        "\n",
        "with open('vectorizer_nlp.pkl', 'rb') as file:\n",
        "    tfidf_vectorize= pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SevX7LqMRYfU"
      },
      "outputs": [],
      "source": [
        "# Load saved models. (Chatbot))\n",
        "\n",
        "# Load embeddings + FAISS index\n",
        "\n",
        "# Load docs + embeddings\n",
        "with open(\"chatbot_store.pkl\", \"rb\") as f:\n",
        "    store = pickle.load(f)\n",
        "\n",
        "docs = store[\"docs\"]\n",
        "embeddings = store[\"embeddings\"]\n",
        "\n",
        "# Load FAISS index\n",
        "index = faiss.read_index(\"chatbot_index.faiss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofEd9qZtmUBg",
        "outputId": "e108972f-0da5-4c75-8a97-c289d6c6ada9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "# Final\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# Load models and scalers\n",
        "# Risk score\n",
        "@st.cache_resource\n",
        "def load_risk_score_model():\n",
        "    with open(\"risk_score_class_xgb_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"scaler_risk_score_class.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "# Claim Amount Models\n",
        "@st.cache_resource\n",
        "def load_claim_amount_ml():\n",
        "    with open(\"claim_amount_reg_gbr_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"scaler_claim_reg.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "# Fraud Detection Model\n",
        "@st.cache_resource\n",
        "def load_fraud_detection_model():\n",
        "    with open(\"fraud_claim_class_lgb_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"scaler_fraud_claim_class.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "@st.cache_resource\n",
        "def load_anomaly_detection_model():\n",
        "    with open(\"fraud_class_ano_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    return model\n",
        "\n",
        "@st.cache_resource\n",
        "def load_anomaly_ml_models():\n",
        "    # Load anomaly detection model\n",
        "    with open(\"fraud_class_ano_model.pkl\", \"rb\") as f:\n",
        "        anomaly_model = pickle.load(f)\n",
        "    # Load ML model for fraud/anomaly classification\n",
        "    with open(\"fraud_class_cm_model.pkl\", \"rb\") as f:\n",
        "        ml_model = pickle.load(f)\n",
        "    # Load scaler if ML model needs scaling\n",
        "    with open(\"scaler_fraud_class_cm_model.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return anomaly_model, ml_model, scaler\n",
        "\n",
        "# Customer Segmentation\n",
        "@st.cache_resource\n",
        "def load_customer_segmentation_model():\n",
        "    with open(\"cus_seg_knn_dbscan.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"cus_seg_scaler.pkl\", \"rb\") as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "\n",
        "# Sentiment Analysis\n",
        "@st.cache_resource\n",
        "def load_sentiment_analysis_model():\n",
        "    with open(\"nlp_model.pkl\", \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    with open(\"vectorizer_nlp.pkl\", \"rb\") as f:\n",
        "        vector = pickle.load(f)\n",
        "    return model, vector\n",
        "\n",
        "# Chatbot\n",
        "@st.cache_resource\n",
        "def load_resources():\n",
        "    # Load dataframe\n",
        "    df = pd.read_csv(\"insurance.csv\")\n",
        "    df = df.fillna(\"\")\n",
        "    df['premium_amount'] = pd.to_numeric(df['premium_amount'], errors='coerce')\n",
        "    df['claim_amount'] = pd.to_numeric(df['claim_amount'], errors='coerce')\n",
        "\n",
        "    # Load docs + embeddings\n",
        "    with open(\"chatbot_store.pkl\", \"rb\") as f:\n",
        "        store = pickle.load(f)\n",
        "\n",
        "    docs = store[\"docs\"]\n",
        "    embeddings = store[\"embeddings\"]\n",
        "\n",
        "    # Load FAISS index\n",
        "    index = faiss.read_index(\"chatbot_index.faiss\")\n",
        "\n",
        "    # Load QA model\n",
        "    qa_model = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=\"google/flan-t5-base\",\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "\n",
        "    embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    # Load translators\n",
        "    translator_hi = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-hi\", device=0 if torch.cuda.is_available() else -1)\n",
        "    translator_fr = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\", device=0 if torch.cuda.is_available() else -1)\n",
        "    translator_es = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    return df, docs, embeddings, index, qa_model, embedder, translator_hi, translator_fr, translator_es\n",
        "\n",
        "df, docs, embeddings, index, qa_model, embedder, translator_hi, translator_fr, translator_es = load_resources()\n",
        "\n",
        "st.title(\"🛡️ Insurance Prediction Dashboard\")\n",
        "st.sidebar.title(\"🔍 Navigation\")\n",
        "main_section = st.sidebar.radio(\n",
        "    \"Choose Section\",\n",
        "    [\"Prediction\", \"Visualization\", \"Chatbot\"]\n",
        ")\n",
        "\n",
        "# ------------\n",
        "# Prediction |\n",
        "# ------------\n",
        "\n",
        "if main_section == \"Prediction\":\n",
        "    problem = st.selectbox(\n",
        "        \"Select Prediction Task\",\n",
        "        [\"Risk Score\", \"Claim Amount\", \"Fraud Detection\", \"Customer Segmentation\", \"Sentiment Analysis\"]\n",
        "    )\n",
        "\n",
        "    # For Risk Score Prediction\n",
        "    if problem == \"Risk Score\":\n",
        "        st.header(\"Risk Score Prediction\")\n",
        "\n",
        "        # ---- Type Input Form ----\n",
        "        customer_age = st.number_input(\"Customer Age\", 18, 100, 45)\n",
        "        monthly_income = st.number_input(\"Monthly Income\", 0, 1_000_000, 57926)\n",
        "        vehicle_or_property_age = st.number_input(\"Vehicle/Property Age\", 0, 50, 6)\n",
        "        claim_history = st.number_input(\"Claim History Count\", 0, 100, 0)\n",
        "        fraudulent_claim = st.selectbox(\"Fraudulent Claim\", [\"No\", \"Yes\"])\n",
        "        premium_amount = st.number_input(\"Premium Amount\", 0, 1_000_000, 1100)\n",
        "        claim_amount = st.number_input(\"Claim Amount\", 0, 1_000_000, 719)\n",
        "        gender = st.selectbox(\"Gender\", [\"Female\", \"Male\", \"Other\"])\n",
        "        policy_type = st.selectbox(\"Policy Type\", [\"Auto\", \"Health\", \"Life\", \"Property\"])\n",
        "\n",
        "        # ---- Encoding ----\n",
        "        fraudulent_claim_enc = 1 if fraudulent_claim == \"Yes\" else 0\n",
        "        gender_encodings = {\"Female\": [1,0,0], \"Male\": [0,1,0], \"Other\": [0,0,1]}\n",
        "        policy_type_encodings = {\n",
        "            \"Auto\": [1,0,0,0], \"Health\": [0,1,0,0], \"Life\": [0,0,1,0], \"Property\": [0,0,0,1]\n",
        "        }\n",
        "\n",
        "        input_features = [\n",
        "            customer_age,\n",
        "            monthly_income,\n",
        "            vehicle_or_property_age,\n",
        "            claim_history,\n",
        "            fraudulent_claim_enc,\n",
        "            premium_amount,\n",
        "            claim_amount,\n",
        "            *gender_encodings[gender],\n",
        "            *policy_type_encodings[policy_type]\n",
        "        ]\n",
        "\n",
        "        columns = [\n",
        "            'customer_age', 'monthly_income', 'vehicle_or_property_age', 'claim_history',\n",
        "            'fraudulent_claim', 'premium_amount', 'claim_amount',\n",
        "            'gender_Female', 'gender_Male', 'gender_Other',\n",
        "            'policy_type_Auto', 'policy_type_Health', 'policy_type_Life', 'policy_type_Property'\n",
        "        ]\n",
        "\n",
        "        input_df = pd.DataFrame([input_features], columns=columns)\n",
        "\n",
        "        if st.button(\"Predict Risk Score\"):\n",
        "          model, scaler = load_risk_score_model()\n",
        "          input_scaled = scaler.transform(input_df)\n",
        "          pred = model.predict(input_scaled)[0]\n",
        "          risk_map = {0: \"LOW\", 1: \"MEDIUM\", 2: \"HIGH\"}\n",
        "          predicted_label = risk_map.get(pred, \"Unknown\")\n",
        "\n",
        "          # Color map for predictions\n",
        "          color_map = {\"LOW\": \"green\",\n",
        "                      \"MEDIUM\": \"orange\",\n",
        "                      \"HIGH\": \"red\"}\n",
        "\n",
        "          color = color_map.get(predicted_label, \"black\")\n",
        "\n",
        "          st.markdown(\n",
        "              f\"<h4>Predicted Risk Level: <span style='color:{color}'>{predicted_label}</span></h4>\",\n",
        "              unsafe_allow_html=True)\n",
        "\n",
        "    # For Claim Amount Prediction\n",
        "\n",
        "    elif problem == \"Claim Amount\":\n",
        "        st.header(\"Claim Amount Prediction\")\n",
        "\n",
        "        # Inputs\n",
        "        customer_age = st.number_input(\"Customer Age\", 18, 100, 45)\n",
        "        monthly_income = st.number_input(\"Monthly Income\", 0, 1_000_000, 57926)\n",
        "        vehicle_or_property_age = st.number_input(\"Vehicle/Property Age\", 0, 50, 6)\n",
        "        claim_history = st.number_input(\"Claim History Count\", 0, 100, 0)\n",
        "        fraudulent_claim = st.selectbox(\"Fraudulent Claim\", [\"No\", \"Yes\"])\n",
        "        premium_amount = st.number_input(\"Premium Amount\", 0, 1_000_000, 1100)\n",
        "        risk_score = st.number_input(\"Risk Score\", 0, 2, 1)\n",
        "\n",
        "        gender = st.selectbox(\"Gender\", [\"Female\", \"Male\", \"Other\"])\n",
        "        policy_type = st.selectbox(\"Policy Type\", [\"Auto\", \"Health\", \"Life\", \"Property\"])\n",
        "\n",
        "        # Encoding\n",
        "        fraudulent_claim_enc = 1 if fraudulent_claim == \"Yes\" else 0\n",
        "        gender_encodings = {\"Female\": [1,0,0], \"Male\": [0,1,0], \"Other\": [0,0,1]}\n",
        "        policy_type_encodings = {\n",
        "            \"Auto\": [1,0,0,0], \"Health\": [0,1,0,0], \"Life\": [0,0,1,0], \"Property\": [0,0,0,1]\n",
        "        }\n",
        "\n",
        "        input_features = [\n",
        "            customer_age,\n",
        "            monthly_income,\n",
        "            vehicle_or_property_age,\n",
        "            claim_history,\n",
        "            fraudulent_claim_enc,\n",
        "            premium_amount,\n",
        "            risk_score,\n",
        "            *gender_encodings[gender],\n",
        "            *policy_type_encodings[policy_type]\n",
        "        ]\n",
        "\n",
        "        columns = [\n",
        "            'customer_age', 'monthly_income', 'vehicle_or_property_age', 'claim_history',\n",
        "            'fraudulent_claim', 'premium_amount', 'risk_score',\n",
        "            'gender_Female', 'gender_Male', 'gender_Other',\n",
        "            'policy_type_Auto', 'policy_type_Health', 'policy_type_Life', 'policy_type_Property'\n",
        "        ]\n",
        "\n",
        "        input_df = pd.DataFrame([input_features], columns=columns)\n",
        "\n",
        "\n",
        "        # Model selection\n",
        "        if st.button(\"Predict Claim Amount\"):\n",
        "            model, scaler = load_claim_amount_ml()\n",
        "            scaled_input = scaler.transform(input_df)\n",
        "            prediction = model.predict(scaled_input)[0]\n",
        "\n",
        "            st.success(f\"Predicted Claim Amount: {prediction:,.2f}\")\n",
        "\n",
        "    # For Fraud Detection\n",
        "    elif problem == \"Fraud Detection\":\n",
        "        st.header(\"Fraud Detection\")\n",
        "\n",
        "        sub_problem = st.radio(\"Select Sub-Category\",\n",
        "                              [\"Normal Fraud Detection\", \"Anomaly Detection\", \"Anomaly Detection with ML\"])\n",
        "\n",
        "        # Common Inputs\n",
        "        customer_age = st.number_input(\"Customer Age\", 18, 100, 45)\n",
        "        monthly_income = st.number_input(\"Monthly Income\", 0, 1_000_000, 57926)\n",
        "        vehicle_or_property_age = st.number_input(\"Vehicle/Property Age\", 0, 50, 6)\n",
        "        claim_history = st.number_input(\"Claim History Count\", 0, 100, 0)\n",
        "        premium_amount = st.number_input(\"Premium Amount\", 0, 1_000_000, 1100)\n",
        "        claim_amount = st.number_input(\"Claim Amount\", 0, 1_000_000, 719)\n",
        "        risk_score = st.number_input(\"Risk Score\", 0, 2, 1)  # 0=LOW, 1=MEDIUM, 2=HIGH\n",
        "        gender = st.selectbox(\"Gender\", [\"Female\", \"Male\", \"Other\"])\n",
        "        policy_type = st.selectbox(\"Policy Type\", [\"Auto\", \"Health\", \"Life\", \"Property\"])\n",
        "\n",
        "        gender_encodings = {\"Female\": [1,0,0], \"Male\": [0,1,0], \"Other\": [0,0,1]}\n",
        "        policy_type_encodings = {\n",
        "            \"Auto\": [1,0,0,0], \"Health\": [0,1,0,0], \"Life\": [0,0,1,0], \"Property\": [0,0,0,1]\n",
        "        }\n",
        "\n",
        "        input_features = [\n",
        "            customer_age,\n",
        "            monthly_income,\n",
        "            vehicle_or_property_age,\n",
        "            claim_history,\n",
        "            premium_amount,\n",
        "            claim_amount,\n",
        "            risk_score,\n",
        "            *gender_encodings[gender],\n",
        "            *policy_type_encodings[policy_type]\n",
        "        ]\n",
        "\n",
        "        columns = [\n",
        "            'customer_age', 'monthly_income', 'vehicle_or_property_age', 'claim_history',\n",
        "            'premium_amount', 'claim_amount', 'risk_score',\n",
        "            'gender_Female', 'gender_Male', 'gender_Other',\n",
        "            'policy_type_Auto', 'policy_type_Health', 'policy_type_Life', 'policy_type_Property'\n",
        "        ]\n",
        "\n",
        "        input_df = pd.DataFrame([input_features], columns=columns)\n",
        "\n",
        "\n",
        "        # --- Normal Fraud Detection ---\n",
        "        if sub_problem == \"Normal Fraud Detection\":\n",
        "            if st.button(\"Predict Fraud\"):\n",
        "                model, scaler = load_fraud_detection_model()\n",
        "                input_scaled = scaler.transform(input_df)\n",
        "                pred = model.predict(input_scaled)[0]\n",
        "                fraud_map = {0: \"NOT FRAUD\", 1: \"FRAUD\"}\n",
        "                predicted_label = fraud_map.get(pred, \"Unknown\")\n",
        "                color_map = {\"NOT FRAUD\": \"green\", \"FRAUD\": \"red\"}\n",
        "                color = color_map.get(predicted_label, \"black\")\n",
        "                st.markdown(\n",
        "                    f\"<h4>Prediction: <span style='color:{color}'>{predicted_label}</span></h4>\",\n",
        "                    unsafe_allow_html=True\n",
        "                )\n",
        "\n",
        "        # --- Anomaly Detection ---\n",
        "        elif sub_problem == \"Anomaly Detection\":\n",
        "            if st.button(\"Detect Anomaly\"):\n",
        "                model = load_anomaly_detection_model()\n",
        "\n",
        "                # Predict (-1 for anomaly, 1 for normal)\n",
        "                anomaly_status = model.predict(input_df)[0]\n",
        "\n",
        "                # Convert to 0/1 for mapping consistency\n",
        "                anomaly_flag = 1 if anomaly_status == -1 else 0\n",
        "                anomaly_map = {0: \"NORMAL\", 1: \"ANOMALY\"}\n",
        "                predicted_label = anomaly_map[anomaly_flag]\n",
        "\n",
        "                # Color coding\n",
        "                color_map = {\"NORMAL\": \"green\", \"ANOMALY\": \"red\"}\n",
        "                color = color_map.get(predicted_label, \"black\")\n",
        "                st.markdown(\n",
        "                    f\"<h4>Anomaly Detection Result: <span style='color:{color}'>{predicted_label}</span></h4>\",\n",
        "                    unsafe_allow_html=True\n",
        "                )\n",
        "\n",
        "        # --- Anomaly Detection with ML ---\n",
        "        elif sub_problem == \"Anomaly Detection with ML\":\n",
        "            st.subheader(\"Anomaly Detection with ML\")\n",
        "\n",
        "            if st.button(\"Run Anomaly + ML Prediction\"):\n",
        "                anomaly_model, ml_model, scaler = load_anomaly_ml_models()\n",
        "\n",
        "                # Step 1: Run anomaly detection\n",
        "                anomaly_status = anomaly_model.predict(input_df)[0]  # -1 = anomaly, 1 = normal\n",
        "                anomaly_flag = 1 if anomaly_status == -1 else 0\n",
        "\n",
        "                anomaly_map = {0: \"NORMAL\", 1: \"ANOMALY\"}\n",
        "                anomaly_label = anomaly_map[anomaly_flag]\n",
        "\n",
        "                # Step 2: Add anomaly flag to features\n",
        "                input_df['anomaly_flag'] = anomaly_flag\n",
        "\n",
        "                # Step 3: Scale for ML model\n",
        "                scaled_input = scaler.transform(input_df)\n",
        "\n",
        "                # Step 4: Predict with ML model\n",
        "                pred_label = ml_model.predict(scaled_input)[0]\n",
        "\n",
        "                result_map = {0: \"LEGITIMATE\", 1: \"FRAUD\"}\n",
        "                ml_label = result_map.get(pred_label, \"Unknown\")\n",
        "\n",
        "                # Colors\n",
        "                color_map = {\"NORMAL\": \"green\", \"ANOMALY\": \"red\"}\n",
        "                color_map_ml = {\"LEGITIMATE\": \"green\", \"FRAUD\": \"red\"}\n",
        "                #color = color_map.get(predicted_label, \"black\")\n",
        "\n",
        "                st.markdown(f\"<h4>Anomaly Detection Result: <span style='color:{color_map[anomaly_label]}'>{anomaly_label}</span></h4>\", unsafe_allow_html=True)\n",
        "                st.markdown(f\"<h4>ML Classification Result: <span style='color:{color_map_ml[ml_label]}'>{ml_label}</span></h4>\", unsafe_allow_html=True)\n",
        "\n",
        "    elif problem == \"Customer Segmentation\":\n",
        "        st.subheader(\"Customer Segmentation\")\n",
        "\n",
        "        # User inputs\n",
        "        customer_age = st.number_input(\"Customer Age\", min_value=18, max_value=100, value=30)\n",
        "        monthly_income = st.number_input(\"Monthly Income\", min_value=0, value=50000)\n",
        "        claim_history = st.number_input(\"Claim History\", min_value=0, value=0)\n",
        "        policy_upgrade = st.number_input(\"Policy Upgrade\", min_value=0, value=7)\n",
        "        num_active_policies = st.number_input(\"Number of Active Policies\", min_value=0, value=1)\n",
        "\n",
        "        if st.button(\"Predict Customer Segment\"):\n",
        "            # Load ML model + scaler\n",
        "            model, scaler = load_customer_segmentation_model()\n",
        "\n",
        "            # Create DataFrame with the same column order as training\n",
        "            input_df = pd.DataFrame([[\n",
        "                customer_age,\n",
        "                monthly_income,\n",
        "                claim_history,\n",
        "                policy_upgrade,\n",
        "                num_active_policies\n",
        "            ]], columns=[\n",
        "                'customer_age',\n",
        "                'monthly_income',\n",
        "                'claim_history',\n",
        "                'policy_upgrade',\n",
        "                'num_active_policies'\n",
        "            ])\n",
        "\n",
        "            # Scale\n",
        "            scaled_input = scaler.transform(input_df)\n",
        "\n",
        "            # Predict\n",
        "            pred_segment = model.predict(scaled_input)[0]\n",
        "\n",
        "            # Mapping\n",
        "            segment_map = {\n",
        "                0: \"Engaged Upgraders\",\n",
        "                1: \"Passive Customers\",\n",
        "                2: \"Upgrade Enthusiasts\"\n",
        "            }\n",
        "            segment_label = segment_map.get(pred_segment, \"Unknown\")\n",
        "\n",
        "            color_map = {\n",
        "                \"Engaged Upgraders\": \"green\",\n",
        "                \"Passive Customers\": \"orange\",\n",
        "                \"Upgrade Enthusiasts\": \"pink\"\n",
        "            }\n",
        "\n",
        "            st.markdown(\n",
        "                f\"<h4>Predicted Customer Segment: <span style='color:{color_map[segment_label]}'>{segment_label}</span></h4>\",\n",
        "                unsafe_allow_html=True\n",
        "            )\n",
        "\n",
        "    # For Sentiment Analysis Prediction\n",
        "    if problem == \"Sentiment Analysis\":\n",
        "        st.header(\"Sentiment Analysis Prediction\")\n",
        "\n",
        "        model, vector = load_sentiment_analysis_model()\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        def clean_text(text):\n",
        "            text = str(text).lower()\n",
        "            text = contractions.fix(text)\n",
        "            text = re.sub(' +', ' ', text)\n",
        "            text = re.sub(r'\\[.*?\\]', '', text)\n",
        "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "            text = re.sub(r'<.*?>+', '', text)\n",
        "            text = re.sub(r'\\n', '', text)\n",
        "            text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "            text = re.sub(r'[^a-z\\s]', '', text)\n",
        "            text = re.sub(r\"\\d+\", \"\", text)\n",
        "            tokens = nltk.word_tokenize(text)\n",
        "            tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "            return \" \".join(tokens)\n",
        "\n",
        "        def predict_sentiment(new_text):\n",
        "            cleaned = clean_text(new_text)\n",
        "            x_tfidf = vector.transform([cleaned])\n",
        "            pred = model.predict(x_tfidf)[0]\n",
        "            return pred\n",
        "\n",
        "        st.write(\"Enter a customer review below to analyze its sentiment:\")\n",
        "\n",
        "        user_input = st.text_area(\"✍️ Customer Review\")\n",
        "\n",
        "        if st.button(\"Predict Sentiment\"):\n",
        "            if user_input.strip() == \"\":\n",
        "                st.warning(\"Please enter some text.\")\n",
        "            else:\n",
        "                prediction = predict_sentiment(user_input)\n",
        "\n",
        "            # Color-coded result\n",
        "                if prediction.lower() == \"positive\":\n",
        "                    st.markdown(f\"**Predicted Sentiment:** <span style='color:green'>{prediction}</span>\", unsafe_allow_html=True)\n",
        "                elif prediction.lower() == \"neutral\":\n",
        "                    st.markdown(f\"**Predicted Sentiment:** <span style='color:blue'>{prediction}</span>\", unsafe_allow_html=True)\n",
        "                elif prediction.lower() == \"negative\":\n",
        "                    st.markdown(f\"**Predicted Sentiment:** <span style='color:red'>{prediction}</span>\", unsafe_allow_html=True)\n",
        "                else:\n",
        "                    st.write(f\"Predicted Sentiment: {prediction}\")  # fallback\n",
        "\n",
        "# ---------------\n",
        "# Visualization |\n",
        "# ---------------\n",
        "\n",
        "\n",
        "elif main_section == \"Visualization\":\n",
        "    st.header(\"📊 Visualization Dashboard\")\n",
        "\n",
        "    # Dataset selector\n",
        "    dataset_choice = st.selectbox(\n",
        "        \"Select Dataset to Visualize\",\n",
        "        [\"Customer Data\",\"Customer Claim\", \"Customer Segments\", \"Customer Reviews\"]\n",
        "    )\n",
        "\n",
        "    # -------------------------------\n",
        "    # Load dataset  - Customer Data |\n",
        "    # -------------------------------\n",
        "    if dataset_choice == \"Customer Data\":\n",
        "      df = pd.read_csv(\"insurance_class_reg_data.csv\")\n",
        "      df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"), inplace=True)\n",
        "      st.subheader(\"📊 Customer Data\")\n",
        "\n",
        "\n",
        "      viz_option = st.radio(\n",
        "          \"Choose Visualization\",\n",
        "          [\"Univariate Analysis (Numerical)\", \"Gender Distribution\", \"Policy Type Distribution\", \"Vehicle or Property Age Distribution\",\n",
        "           \"Claim History Distribution\", \"Fraudulent Claim Distribution\", \"Risk Score Distribution\",\"Location Distribution\",\n",
        "           \"Policy Upgrade Distribution\", \"Claim History by Gender\", \"Monthly Income by Gender\", \"Vehicle/Property Age by Gender\",\n",
        "           \"Policy Type by Gender\",\"Premium Amount by Gender\", \"Claim Amount by Gender\", \"Risk Score by Gender\",\n",
        "           \"Customer Age Density by Gender\",\"Customer Age by Claim History\",\"Fraudulent Claims by Gender\",\n",
        "           \"Fraudulent Claim by Risk Score\",\"Fraudulent Claim by Policy Type\", \"Claim Amount by Risk Score\",\n",
        "           \"Proportion of Risk Score by Gender and Policy Type\", \"Correlation Heatmap\",\n",
        "           \"Fraudulent Claims by Gender and Policy Type\", \"Customer Age Distribution by Policy Type\"]\n",
        "      )\n",
        "\n",
        "      # --- Univariate Analysis (Numerical) ---\n",
        "      if viz_option == \"Univariate Analysis (Numerical)\":\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        num_cols = ['customer_age', 'monthly_income', 'vehicle_or_property_age',\n",
        "                    'claim_amount', 'premium_amount', 'claim_history']\n",
        "\n",
        "        for col in num_cols:\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "            # Histogram\n",
        "            sns.histplot(df[col], bins=10, kde=True, ax=axes[0], color=\"skyblue\")\n",
        "            axes[0].set_title(f\"Histogram of {col}\")\n",
        "\n",
        "            # Boxplot\n",
        "            sns.boxplot(y=df[col], ax=axes[1], color=\"lightcoral\")\n",
        "            axes[1].set_title(f\"Boxplot of {col}\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "      # --- Gender Distribution ---\n",
        "      if viz_option == \"Gender Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='gender', palette='Set2', ax=ax)\n",
        "        ax.set_title(\"Gender Distribution\")\n",
        "        ax.set_xlabel(\"Gender\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Policy Type Distribution ---\n",
        "      if viz_option == \"Policy Type Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='policy_type', palette='Set1', ax=ax)\n",
        "        ax.set_title(\"Policy Type Distribution\")\n",
        "        ax.set_xlabel(\"Policy Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Vehicle or Property Age Distribution ---\n",
        "      if viz_option == \"Vehicle or Property Age Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='vehicle_or_property_age', palette='Set3', ax=ax)\n",
        "        ax.set_title(\"Vehicle or Property Age Distribution\")\n",
        "        ax.set_xlabel(\"Vehicle or Property Age\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "\n",
        "      # --- Claim History Distribution ---\n",
        "      if viz_option == \"Claim History Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='claim_history', palette='Set2', ax=ax)\n",
        "        ax.set_title(\"Claim History Distribution\")\n",
        "        ax.set_xlabel(\"Claim History\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claim Distribution ---\n",
        "      if viz_option == \"Fraudulent Claim Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        sns.countplot(data=df, x='fraudulent_claim', ax=ax)\n",
        "        ax.set_title(\"Fraudulent Claim Distribution\")\n",
        "        ax.set_xlabel(\"Fraudulent Claim (0 = No, 1 = Yes)\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        ax.set_xticks([0, 1])\n",
        "        ax.set_xticklabels(['No Fraud', 'Fraud'])\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Risk Score Distribution ---\n",
        "      if viz_option == \"Risk Score Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        sns.countplot(data=df, x='risk_score', palette='Set2', ax=ax)\n",
        "        ax.set_title(\"Risk Score Distribution\")\n",
        "        ax.set_xlabel(\"Risk Score\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Location Distribution ---\n",
        "      if viz_option == \"Location Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        sns.countplot(data=df, x='location', ax=ax)\n",
        "        ax.set_title(\"Location Distribution\")\n",
        "        ax.set_xlabel(\"Location\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Policy Upgrade Distribution ---\n",
        "      if viz_option == \"Policy Upgrade Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        sns.countplot(data=df, x='policy_upgrade', palette='Set2', ax=ax)\n",
        "        ax.set_title(\"Policy Upgrade Distribution\")\n",
        "        ax.set_xlabel(\"Policy Upgrade\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Claim History by Gender ---\n",
        "      if viz_option == \"Claim History by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.countplot(data=df, x='claim_history', hue='gender', palette='Set2', ax=ax)\n",
        "          ax.set_title(\"Claim History by Gender\")\n",
        "          ax.set_xlabel(\"Claim History\")\n",
        "          ax.set_ylabel(\"Count\")\n",
        "          ax.legend(title='Gender')\n",
        "          plt.xticks(rotation=45)\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Monthly Income by Gender ---\n",
        "      if viz_option == \"Monthly Income by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.boxplot(x='gender', y='monthly_income', data=df, palette='pastel', ax=ax)\n",
        "          ax.set_title(\"Distribution of Monthly Income by Gender\")\n",
        "          ax.set_xlabel(\"Gender\")\n",
        "          ax.set_ylabel(\"Monthly Income\")\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Vehicle/Property Age by Gender ---\n",
        "      if viz_option == \"Vehicle/Property Age by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.kdeplot(data=df, x='vehicle_or_property_age', hue='gender', fill=True, ax=ax)\n",
        "          ax.set_title(\"Distribution of Vehicle or Property Age by Gender\")\n",
        "          ax.set_xlabel(\"Vehicle or Property Age\")\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Policy Type by Gender ---\n",
        "      if viz_option == \"Policy Type by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.countplot(data=df, x='policy_type', hue='gender', palette='Set1', ax=ax)\n",
        "          ax.set_title(\"Policy Type by Gender\")\n",
        "          ax.set_xlabel(\"Policy Type\")\n",
        "          ax.set_ylabel(\"Count\")\n",
        "          ax.legend(title='Gender')\n",
        "          plt.xticks(rotation=45)\n",
        "          plt.tight_layout()\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Premium Amount by Gender ---\n",
        "      if viz_option == \"Premium Amount by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.boxplot(data=df, x='gender', y='premium_amount', palette='Set3', ax=ax)\n",
        "          ax.set_title(\"Premium Amount by Gender\")\n",
        "          ax.set_xlabel(\"Gender\")\n",
        "          ax.set_ylabel(\"Premium Amount\")\n",
        "          plt.tight_layout()\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Claim Amount by Gender ---\n",
        "      if viz_option == \"Claim Amount by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.boxplot(data=df, x='gender', y='claim_amount', palette='pastel', ax=ax)\n",
        "          ax.set_title(\"Claim Amount by Gender\")\n",
        "          ax.set_xlabel(\"Gender\")\n",
        "          ax.set_ylabel(\"Claim Amount\")\n",
        "          plt.tight_layout()\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Risk Score by Gender ---\n",
        "      if viz_option == \"Risk Score by Gender\":\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(7, 4))\n",
        "          sns.countplot(data=df, x='risk_score', hue='gender', palette='Set1', ax=ax)\n",
        "          ax.set_title(\"Gender Distribution Across Risk Scores\")\n",
        "          ax.set_xlabel(\"Risk Score\")\n",
        "          ax.set_ylabel(\"Count\")\n",
        "          ax.legend(title='Gender')\n",
        "          plt.tight_layout()\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # --- Customer Age Density by Gender ---\n",
        "      if viz_option == \"Customer Age Density by Gender\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7, 4))\n",
        "        sns.violinplot(data=df, x='gender', y='customer_age', palette='muted', ax=ax)\n",
        "        ax.set_title(\"Customer Age Density by Gender\")\n",
        "        ax.set_xlabel(\"Gender\")\n",
        "        ax.set_ylabel(\"Customer Age\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Customer Age by Claim History ---\n",
        "      if viz_option == \"Customer Age by Claim History\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7, 4))\n",
        "        sns.boxplot(data=df, x='claim_history', y='customer_age', palette='Set1', ax=ax)\n",
        "        ax.set_title(\"Customer Age Distribution by Claim History\")\n",
        "        ax.set_xlabel(\"Claim History\")\n",
        "        ax.set_ylabel(\"Customer Age\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claims by Gender ---\n",
        "      if viz_option == \"Fraudulent Claims by Gender\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        sns.countplot(\n",
        "            data=df,\n",
        "            x='fraudulent_claim',\n",
        "            hue='gender',\n",
        "            palette='Set2',\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(\"Fraudulent Claims by Gender\")\n",
        "        ax.set_xlabel(\"Fraudulent Claim (0 = No, 1 = Yes)\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        ax.legend(title='Gender')\n",
        "        ax.set_xticks([0, 1])\n",
        "        ax.set_xticklabels(['No Fraud', 'Fraud'])\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claim by Risk Score ---\n",
        "      if viz_option == \"Fraudulent Claim by Risk Score\":\n",
        "\n",
        "        df1 = df[['risk_score','fraudulent_claim']].copy()\n",
        "        df1['fraudulent_claim'] = df1['fraudulent_claim'].astype(str)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7,4))\n",
        "        sns.countplot(data=df1, x='risk_score', hue='fraudulent_claim', ax=ax, palette=\"Set2\")\n",
        "        ax.set_title(\"Fraudulent Claim by Risk Score\")\n",
        "        ax.set_xlabel(\"Risk Score\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claim by Policy Type ---\n",
        "      if viz_option == \"Fraudulent Claim by Policy Type\":\n",
        "\n",
        "        df1 = df[['fraudulent_claim', 'policy_type']].copy()\n",
        "        df1['fraudulent_claim'] = df1['fraudulent_claim'].astype(str)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7,4))\n",
        "        sns.countplot(data=df1, x='policy_type', hue='fraudulent_claim', palette=\"coolwarm\", ax=ax)\n",
        "        ax.set_title(\"Fraudulent Claim by Policy Type\")\n",
        "        ax.set_xlabel(\"Policy Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Claim Amount by Risk Score ---\n",
        "      if viz_option == \"Claim Amount by Risk Score\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7,4))\n",
        "        sns.boxplot(data=df, x='risk_score', y='claim_amount', palette=\"Set1\", ax=ax)\n",
        "        ax.set_title(\"Claim Amount by Risk Score\")\n",
        "        ax.set_xlabel(\"Risk Score\")\n",
        "        ax.set_ylabel(\"Claim Amount\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Proportion of Risk Score by Gender and Policy Type ---\n",
        "      if viz_option == \"Proportion of Risk Score by Gender and Policy Type\":\n",
        "\n",
        "        grouped = df.groupby(['gender', 'policy_type', 'risk_score']).size().reset_index(name='count')\n",
        "        pivot_df = grouped.pivot_table(\n",
        "            index=['gender', 'policy_type'],\n",
        "            columns='risk_score',\n",
        "            values='count',\n",
        "            fill_value=0\n",
        "        )\n",
        "        pivot_df_percent = pivot_df.div(pivot_df.sum(axis=1), axis=0)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10,6))\n",
        "        pivot_df_percent.plot(\n",
        "            kind='bar',\n",
        "            stacked=True,\n",
        "            ax=ax,\n",
        "            colormap='coolwarm'\n",
        "        )\n",
        "        ax.set_title(\"Proportion of Risk Score by Gender and Policy Type\")\n",
        "        ax.set_ylabel(\"Proportion\")\n",
        "        ax.legend(title=\"Risk Score\", labels=[\"Low\", \"Medium\", \"High\"])\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Correlation Heatmap --\n",
        "      if viz_option == \"Correlation Heatmap\":\n",
        "\n",
        "        numerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        sns.heatmap(\n",
        "            df[numerical_cols].corr(),\n",
        "            annot=True,\n",
        "            fmt=\".0%\",\n",
        "            cmap=\"YlGnBu\",\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(\"Correlation Heatmap of Numerical Features\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Fraudulent Claims by Gender and Policy Type ---\n",
        "      if viz_option == \"Fraudulent Claims by Gender and Policy Type\":\n",
        "\n",
        "        df1 = df[['gender', 'policy_type', 'fraudulent_claim']].copy()\n",
        "        df1['fraudulent_claim'] = df1['fraudulent_claim'].astype(str)\n",
        "        df1['gender_policy'] = df1['gender'] + \" - \" + df1['policy_type']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        sns.countplot(\n",
        "            data=df1,\n",
        "            x='gender_policy',\n",
        "            hue='fraudulent_claim',\n",
        "            palette='coolwarm',\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(\"Fraudulent Claims by Gender and Policy Type\")\n",
        "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "        ax.set_xlabel(\"Gender - Policy Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        ax.legend(title=\"Fraudulent Claim\", labels=[\"Not Fraud\", \"Fraud\"])\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      # --- Customer Age Distribution by Policy Type ---\n",
        "      if viz_option == \"Customer Age Distribution by Policy Type\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "        sns.kdeplot(\n",
        "            data=df,\n",
        "            x='customer_age',\n",
        "            hue='policy_type',\n",
        "            fill=True,\n",
        "            ax=ax\n",
        "        )\n",
        "        ax.set_title(\"Customer Age Distribution by Policy Type\")\n",
        "        ax.set_xlabel(\"Customer Age\")\n",
        "        ax.set_ylabel(\"Density\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Load dataset - Customer Claim |\n",
        "    # -------------------------------\n",
        "\n",
        "    if dataset_choice == \"Customer Claim\":\n",
        "      df = pd.read_csv(\"claim_base_data.csv\")\n",
        "      df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"), inplace=True)\n",
        "      st.subheader(\"📊 Customer Claim Data\")\n",
        "\n",
        "\n",
        "      # Convert claim_date to datetime and extract Year + Month\n",
        "      df['claim_date'] = pd.to_datetime(df['claim_date'])\n",
        "      df['Year'] = df['claim_date'].dt.year\n",
        "      df['Month'] = df['claim_date'].dt.to_period('M')\n",
        "\n",
        "      # Let user pick a Year\n",
        "      years = sorted(df['Year'].unique())\n",
        "      selected_year = st.selectbox(\"Select Year\", years)\n",
        "\n",
        "      # Filter by selected year\n",
        "      df_year = df[df['Year'] == selected_year]\n",
        "\n",
        "      # Group by month\n",
        "      monthly_counts = df_year.groupby('Month').size()\n",
        "\n",
        "      # Plot\n",
        "      fig, ax = plt.subplots(figsize=(12, 4))\n",
        "      monthly_counts.plot(kind='bar', ax=ax, color=\"skyblue\")\n",
        "\n",
        "      ax.set_title(f\"Monthly Claim Counts ({selected_year})\")\n",
        "      ax.set_xlabel(\"Month\")\n",
        "      ax.set_ylabel(\"Number of Claims\")\n",
        "\n",
        "      plt.xticks(rotation=45)\n",
        "      st.pyplot(fig)\n",
        "\n",
        "    # ----------------------------------\n",
        "    # Load dataset - Customer Segments |\n",
        "    # ----------------------------------\n",
        "\n",
        "    if dataset_choice == \"Customer Segments\":\n",
        "      df = pd.read_csv(\"cus_seg.csv\")\n",
        "      df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"), inplace=True)\n",
        "      st.subheader(\"📊 Customer Segments Data\")\n",
        "\n",
        "      viz_option = st.radio(\n",
        "          \"Choose Visualization\",\n",
        "          [\"DBSCAN Cluster Distribution\", \"Monthly Income Distribution by DBSCAN Cluster\",\n",
        "           \"Claim History Distribution Across DBSCAN Clusters\", \"Customer Segments in 3D (PCA + DBSCAN)\"]\n",
        "          )\n",
        "\n",
        "\n",
        "      if viz_option == \"DBSCAN Cluster Distribution\":\n",
        "\n",
        "        # Plot DBSCAN cluster distribution\n",
        "        fig, ax = plt.subplots(figsize=(7,4))\n",
        "        df['dbscan_cluster'].value_counts().sort_index().plot(\n",
        "            kind='bar',\n",
        "            color=['lightcoral', 'orange', 'pink', 'skyblue'],  # custom colors\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        ax.set_title(\"Cluster Distribution\")\n",
        "        ax.set_xlabel(\"DBSCAN Cluster\")\n",
        "        ax.set_ylabel(\"Number of Customers\")\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Monthly Income Distribution by DBSCAN Cluster\":\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        # KDE plot\n",
        "        sns.kdeplot(\n",
        "          data=df,\n",
        "          x='monthly_income',\n",
        "          hue='dbscan_cluster',\n",
        "          common_norm=False,\n",
        "          ax=ax,\n",
        "          fill=True,   # Optional: fills under the curve for clarity\n",
        "          alpha=0.5    # Optional: makes it slightly transparent\n",
        "        )\n",
        "\n",
        "        ax.set_title(\"Distribution of Monthly Income by DBSCAN Cluster\", fontsize=14)\n",
        "        ax.set_xlabel(\"Monthly Income\")\n",
        "        ax.set_ylabel(\"Density\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Display in Streamlit\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Claim History Distribution Across DBSCAN Clusters\":\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "        # Boxplot\n",
        "        sns.boxplot(\n",
        "            data=df,\n",
        "            x='dbscan_cluster',\n",
        "            y='claim_history',\n",
        "            ax=ax,\n",
        "            palette=\"Set2\"   # optional for colors\n",
        "        )\n",
        "\n",
        "        ax.set_title(\"Claim History Distribution Across DBSCAN Clusters\", fontsize=14)\n",
        "        ax.set_xlabel(\"DBSCAN Cluster\")\n",
        "        ax.set_ylabel(\"Claim History\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Display in Streamlit\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Customer Segments in 3D (PCA + DBSCAN)\":\n",
        "\n",
        "        fig = px.scatter_3d(\n",
        "            df,\n",
        "            x='pc1',\n",
        "            y='pc2',\n",
        "            z='pc3',\n",
        "            color='dbscan_cluster',  # Cluster label\n",
        "            title='Customer Segments in 3D (PCA + DBSCAN)',\n",
        "            labels={\n",
        "                'PC1': 'Principal Component 1',\n",
        "                'PC2': 'Principal Component 2',\n",
        "                'PC3': 'Principal Component 3'\n",
        "            },\n",
        "            opacity=0.8\n",
        "        )\n",
        "\n",
        "        # Update marker size\n",
        "        fig.update_traces(marker=dict(size=5))\n",
        "\n",
        "        # Layout settings\n",
        "        fig.update_layout(\n",
        "            template='plotly_white',\n",
        "            legend_title=\"Cluster\"\n",
        "        )\n",
        "\n",
        "        # Show in Streamlit\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "\n",
        "    # ---------------------------------\n",
        "    # Load dataset - Customer Reviews |\n",
        "    # ---------------------------------\n",
        "\n",
        "\n",
        "    if dataset_choice == \"Customer Reviews\":\n",
        "      df = pd.read_csv(\"reviews.csv\")\n",
        "      df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"), inplace=True)\n",
        "      st.subheader(\"📊 Customer Reviews Data\")\n",
        "\n",
        "      viz_option = st.radio(\n",
        "          \"Choose Visualization\",\n",
        "          [\"Sentiment Distribution\", \"Rating Distribution\", \"Rating Distribution by Service Type Status\", \"WordClouds by Sentiment\"]\n",
        "          )\n",
        "\n",
        "      if viz_option == \"Sentiment Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6,4))\n",
        "        sns.countplot(data=df, x=\"sentiment_label\", palette=\"Set2\", ax=ax)\n",
        "        ax.set_title(\"Sentiment Distribution\")\n",
        "        ax.set_xlabel(\"Sentiment Label\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Rating Distribution\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        sns.countplot(data=df, x='rating', palette='coolwarm', ax=ax)\n",
        "\n",
        "        ax.set_title('Rating Distribution')\n",
        "        ax.set_xlabel('Rating')\n",
        "        ax.set_ylabel('Count')\n",
        "\n",
        "        # Add counts on top of bars\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            ax.text(p.get_x() + p.get_width()/2., height + 1, int(height), ha=\"center\")\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "      if viz_option == \"Rating Distribution by Service Type Status\":\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        sns.countplot(data=df, x='rating', hue='service_type', ax=ax)\n",
        "\n",
        "        ax.set_title('Rating Distribution by Service Type Status')\n",
        "        ax.set_xlabel('Rating')\n",
        "        ax.set_ylabel('Count')\n",
        "\n",
        "        # Add counts on top of bars\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            if height > 0:\n",
        "                ax.text(\n",
        "                    p.get_x() + p.get_width()/2.,\n",
        "                    height + 1,\n",
        "                    int(height),\n",
        "                    ha=\"center\",\n",
        "                    fontsize=9\n",
        "                )\n",
        "\n",
        "        ax.legend(title='Service Type')\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "\n",
        "      def plot_wordcloud(sentiment, color=\"viridis\"):\n",
        "          text = \" \".join(df[df[\"sentiment_label\"] == sentiment][\"clean_text\"])\n",
        "\n",
        "          if not text.strip():  # If no text available\n",
        "              st.warning(f\"No reviews found for {sentiment} sentiment.\")\n",
        "              return\n",
        "\n",
        "          wc = WordCloud(\n",
        "              width=800,\n",
        "              height=400,\n",
        "              background_color=\"white\",\n",
        "              colormap=color,\n",
        "              max_words=200\n",
        "          ).generate(text)\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(10, 5))\n",
        "          ax.imshow(wc, interpolation=\"bilinear\")\n",
        "          ax.axis(\"off\")\n",
        "          ax.set_title(f\"WordCloud for {sentiment} Reviews\", fontsize=16)\n",
        "\n",
        "          st.pyplot(fig)\n",
        "\n",
        "      # Example usage inside your Streamlit viz section\n",
        "\n",
        "      if viz_option == \"WordClouds by Sentiment\":\n",
        "          st.subheader(\"WordClouds by Sentiment\")\n",
        "          plot_wordcloud(\"Positive\", color=\"Greens\")\n",
        "          plot_wordcloud(\"Neutral\", color=\"Blues\")\n",
        "          plot_wordcloud(\"Negative\", color=\"Reds\")\n",
        "\n",
        "# ---------\n",
        "# Chatbot |\n",
        "# ---------\n",
        "\n",
        "if main_section == \"Chatbot\":\n",
        "    # -----------------------\n",
        "    # Row to Text (for RAG) |\n",
        "    # -----------------------\n",
        "\n",
        "    def row_to_text(row):\n",
        "        risk_map = {\"high\": \"High Risk\", \"medium\": \"Medium Risk\", \"low\": \"Low Risk\"}\n",
        "        fraud_flag = \"Fraudulent Claim\" if row['fraudulent_claim'] == 1 else \"Non-Fraudulent Claim\"\n",
        "        sentiment_map = {\n",
        "            \"positive\": \"😊 Positive\",\n",
        "            \"negative\": \"😞 Negative\",\n",
        "            \"neutral\": \"😐 Neutral\"\n",
        "        }\n",
        "\n",
        "        risk_level = risk_map.get(str(row['risk_score']).lower(), str(row['risk_score']))\n",
        "        sentiment_text = sentiment_map.get(str(row['sentiment_label']).lower(), str(row['sentiment_label']))\n",
        "\n",
        "        doc = f\"\"\"\n",
        "        Claim {row['claim_id']} | Customer {row['customer_id']} ({row['gender']}, {row['customer_age']} yrs, {row['location']})\n",
        "        Policy {row['policy_id']} | Type: {row['policy_type']}\n",
        "        Premium: {row['premium_amount']} | Claim Amount: {row['claim_amount']}\n",
        "        Risk: {risk_level} | Fraud: {fraud_flag}\n",
        "        Upgrade: {row['policy_upgrade']} | Active Policies: {row['num_active_policies']}\n",
        "        Review: {row['review_text']} | Sentiment: {sentiment_text} | Rating: {row['rating']}\n",
        "        Policy EN: {row['policy_text_en']}\n",
        "        \"\"\"\n",
        "        return \" \".join(doc.split())\n",
        "\n",
        "\n",
        "    # ---------------\n",
        "    # Chatbot Query |\n",
        "    # ---------------\n",
        "\n",
        "    def chatbot_query(query, top_k=3):\n",
        "        query_clean = query.strip().lower()\n",
        "        query_upper = query.strip().upper()\n",
        "\n",
        "        # --- Case 1: Direct ID Search ---\n",
        "        if query_upper.startswith(\"CUST\"):\n",
        "            match = df[df['customer_id'].str.upper() == query_upper]\n",
        "\n",
        "            # 🔥 If customer has multiple policies → list them\n",
        "            if not match.empty:\n",
        "                policies = match['policy_id'].unique().tolist()\n",
        "                answer = f\"Customer {query_upper} has {len(policies)} policy(s): {', '.join(policies)}\"\n",
        "            else:\n",
        "                answer = f\"No records found for Customer {query_upper}.\"\n",
        "\n",
        "        elif query_upper.startswith(\"CLM\"):\n",
        "            match = df[df['claim_id'].str.upper() == query_upper]\n",
        "            if not match.empty:\n",
        "                docs_to_summarize = [row_to_text(row) for _, row in match.iterrows()]\n",
        "                context = \"\\n\\n\".join(docs_to_summarize)\n",
        "                llm_input = f\"Summarize this claim info:\\n{context}\"\n",
        "                answer = qa_model(llm_input, max_new_tokens=200)[0]['generated_text']\n",
        "            else:\n",
        "                answer = f\"No records found for Claim {query_upper}.\"\n",
        "\n",
        "        elif query_upper.startswith(\"POL\"):\n",
        "          # Extract only the policy ID (first word like \"POL100006\")\n",
        "          policy_id = query_upper.split()[0]\n",
        "\n",
        "          match = df[df['policy_id'].str.upper() == policy_id]\n",
        "          if not match.empty:\n",
        "              # --- Language check ---\n",
        "              if \"hindi\" in query_clean or \"हिंदी\" in query_clean:\n",
        "                  answer = f\"Policy {policy_id} (Hindi):\\n{match.iloc[0]['policy_text_hi']}\"\n",
        "              elif \"french\" in query_clean or \"français\" in query_clean:\n",
        "                  answer = f\"Policy {policy_id} (French):\\n{match.iloc[0]['policy_text_fr']}\"\n",
        "              elif \"spanish\" in query_clean or \"español\" in query_clean:\n",
        "                  answer = f\"Policy {policy_id} (Spanish):\\n{match.iloc[0]['policy_text_es']}\"\n",
        "              else:\n",
        "                  # Default → English summary\n",
        "                  docs_to_summarize = [row_to_text(row) for _, row in match.iterrows()]\n",
        "                  context = \"\\n\\n\".join(docs_to_summarize)\n",
        "                  llm_input = f\"Summarize this policy info:\\n{context}\"\n",
        "                  answer = qa_model(llm_input, max_new_tokens=200)[0]['generated_text']\n",
        "          else:\n",
        "              answer = f\"No records found for Policy {policy_id}.\"\n",
        "\n",
        "        else:\n",
        "\n",
        "            # --- Case 2: Stats Queries ---\n",
        "            if \"unique customer\" in query_clean:\n",
        "                answer = f\"There are {df['customer_id'].nunique()} unique customers.\"\n",
        "            elif \"unique policy\" in query_clean:\n",
        "                answer = f\"There are {df['policy_id'].nunique()} unique policies.\"\n",
        "            elif \"unique claim\" in query_clean:\n",
        "                answer = f\"There are {df['claim_id'].nunique()} unique claims.\"\n",
        "            elif \"fraudulent\" in query_clean:\n",
        "                fraud_count = df[df['fraudulent_claim'] == 1].shape[0]\n",
        "                nonfraud_count = df[df['fraudulent_claim'] == 0].shape[0]\n",
        "                answer = f\"There are {fraud_count} fraudulent claims and {nonfraud_count} non-fraudulent claims.\"\n",
        "            elif \"average premium\" in query_clean:\n",
        "                avg_premium = df['premium_amount'].mean()\n",
        "                answer = f\"The average premium amount is {avg_premium:.2f}.\"\n",
        "            elif \"average claim\" in query_clean or \"average claim amount\" in query_clean:\n",
        "                avg_claim = df['claim_amount'].mean()\n",
        "                answer = f\"The average claim amount is {avg_claim:.2f}.\"\n",
        "            elif \"total premium\" in query_clean:\n",
        "                total_premium = df['premium_amount'].sum()\n",
        "                answer = f\"The total premium collected is {total_premium:.2f}.\"\n",
        "            elif \"total claim\" in query_clean:\n",
        "                total_claim = df['claim_amount'].sum()\n",
        "                answer = f\"The total claim amount is {total_claim:.2f}.\"\n",
        "\n",
        "            # --- New: Risk-level statistics ---\n",
        "            elif \"risk breakdown\" in query_clean:\n",
        "                stats = []\n",
        "                for risk in [\"Low\", \"Medium\", \"High\"]:\n",
        "                    sub = df[df['risk_score'].str.lower() == risk.lower()]\n",
        "                    frauds = sub[sub['fraudulent_claim'] == 1].shape[0]\n",
        "                    nonfrauds = sub[sub['fraudulent_claim'] == 0].shape[0]\n",
        "                    total_premium = sub['premium_amount'].sum()\n",
        "                    total_claim = sub['claim_amount'].sum()\n",
        "                    stats.append(\n",
        "                        f\"{risk} Risk → {frauds} fraudulent, {nonfrauds} non-fraudulent, \"\n",
        "                        f\"Total Premium: {total_premium:.2f}, Total Claim: {total_claim:.2f}\"\n",
        "                    )\n",
        "                answer = \"Risk Level Breakdown:\\n\" + \"\\n\".join(stats)\n",
        "\n",
        "            else:\n",
        "\n",
        "                # --- Case 3: Semantic Search ---\n",
        "                query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "                D, I = index.search(query_embedding, top_k)\n",
        "                retrieved_docs = [docs[idx] for idx in I[0] if idx < len(docs)]\n",
        "                context = \"\\n\\n\".join(retrieved_docs)\n",
        "                llm_input = f\"Answer based on this context:\\n{context}\\n\\nUser query: {query}\"\n",
        "                answer = qa_model(llm_input, max_new_tokens=200)[0]['generated_text']\n",
        "\n",
        "        # --- Language Detection --- (remove translation if already in target lang)\n",
        "        if (\"hindi\" in query_clean or \"हिंदी\" in query_clean) and not query_upper.startswith(\"POL\"):\n",
        "            return \"HI \" + translator_hi(answer)[0]['translation_text']\n",
        "        elif (\"french\" in query_clean or \"français\" in query_clean) and not query_upper.startswith(\"POL\"):\n",
        "            return \"FR \" + translator_fr(answer)[0]['translation_text']\n",
        "        elif (\"spanish\" in query_clean or \"español\" in query_clean) and not query_upper.startswith(\"POL\"):\n",
        "            return \"ES \" + translator_es(answer)[0]['translation_text']\n",
        "        else:\n",
        "            return \"EN \" + answer\n",
        "\n",
        "    # -----------------------------\n",
        "    # Streamlit UI\n",
        "    # -----------------------------\n",
        "    st.title(\"🛡️ Insurance Chatbot\")\n",
        "    st.write(\"Ask me about policies, claims, or customers!\")\n",
        "\n",
        "    query = st.text_input(\"Enter your query:\")\n",
        "    if st.button(\"Ask\") and query:\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            answer = chatbot_query(query)\n",
        "        st.success(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnSP-ijlhHn_",
        "outputId": "21b6a450-ecb0-4e03-e01e-1883f82b6d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok.zip\n",
            "  inflating: /usr/local/bin/ngrok    \n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Download and install the latest ngrok\n",
        "!wget -qO ngrok.zip https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -o ngrok.zip -d /usr/local/bin && chmod +x /usr/local/bin/ngrok\n",
        "\n",
        "# Add your ngrok authentication token (replace below with your token)\n",
        "!ngrok config add-authtoken 2xJRPyoirSy9D8YpdP88rm6tWgH_6ocNEUgvkuU6Zyh5xn8NS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kWjDDCMfw5X",
        "outputId": "33bc503b-7cc1-4ae1-d3c1-0365977f84db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Your Streamlit app is live here:\n",
            "https://0fa18b319846.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py --server.headless true\")\n",
        "\n",
        "# Start Streamlit app in a background thread\n",
        "threading.Thread(target=run_streamlit).start()\n",
        "time.sleep(5)  # wait for Streamlit to start\n",
        "\n",
        "# Start ngrok tunnel in background\n",
        "os.system(\"ngrok http 8501 &\")\n",
        "time.sleep(5)  # wait for ngrok to start\n",
        "\n",
        "# Get the public URL\n",
        "try:\n",
        "    tunnels = requests.get(\"http://localhost:4040/api/tunnels\").json()\n",
        "    public_url = tunnels['tunnels'][0]['public_url']\n",
        "    print(\"🌐 Your Streamlit app is live here:\")\n",
        "    print(public_url)\n",
        "except Exception as e:\n",
        "    print(\"❌ Error getting ngrok URL:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E95Yz0zgJm3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gKwvy-NGJoP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rUOOvTmGJoXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2TkWkODWx8K"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SULYdSx0MnCN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CksJI7CjApTJ",
        "outputId": "b8aa0457-c984-4d5b-9226-518bc35d0653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2xJRPyoirSy9D8YpdP88rm6tWgH_6ocNEUgvkuU6Zyh5xn8NS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmgHmAlD6NIf"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import threading, os\n",
        "\n",
        "# Kill any previous tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a tunnel on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🌍 Public URL:\", public_url)\n",
        "\n",
        "# Run Streamlit in a background thread\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py --server.port 8501 --server.headless true\")\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hms3q8fPWx-w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woGPS9fPXg7W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7U6tZFDXg_n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX3JuKsJX5jr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiiI30WfX5mf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PntcQQoTX5pL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lShxoQGtwMVG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd50+O2S00CYAfVpEJChyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}