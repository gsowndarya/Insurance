{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68541d8-42b8-40e6-8bc1-6fadfd43c6d0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4181459c-906b-4176-8e45-7b71f09d62dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading faker-37.4.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tzdata in c:\\users\\admin\\anaconda3\\lib\\site-packages (from faker) (2023.3)\n",
      "Downloading faker-37.4.2-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.9 MB 660.6 kB/s eta 0:00:03\n",
      "    --------------------------------------- 0.0/1.9 MB 660.6 kB/s eta 0:00:03\n",
      "    --------------------------------------- 0.0/1.9 MB 330.3 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.9 MB 359.3 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.9 MB 375.8 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.1/1.9 MB 347.8 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.9 MB 380.5 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.1/1.9 MB 358.2 kB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.2/1.9 MB 367.6 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.2/1.9 MB 374.9 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.2/1.9 MB 374.9 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.2/1.9 MB 374.9 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.2/1.9 MB 374.9 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.2/1.9 MB 374.9 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 387.5 kB/s eta 0:00:05\n",
      "   ------ --------------------------------- 0.3/1.9 MB 399.3 kB/s eta 0:00:05\n",
      "   ------ --------------------------------- 0.3/1.9 MB 401.6 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.3/1.9 MB 408.1 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.4/1.9 MB 417.2 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.4/1.9 MB 414.2 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.4/1.9 MB 423.1 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.4/1.9 MB 419.8 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 0.5/1.9 MB 414.6 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 0.5/1.9 MB 412.2 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 0.5/1.9 MB 413.2 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 419.7 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 412.0 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 407.7 kB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 0.5/1.9 MB 405.9 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.6/1.9 MB 412.5 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.6/1.9 MB 410.8 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.6/1.9 MB 411.6 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 0.6/1.9 MB 409.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.6/1.9 MB 410.7 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.7/1.9 MB 411.4 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.7/1.9 MB 405.9 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.7/1.9 MB 406.7 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.7/1.9 MB 401.6 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.7/1.9 MB 402.5 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.7/1.9 MB 403.3 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.8/1.9 MB 398.7 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.8/1.9 MB 398.7 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.8/1.9 MB 398.7 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.8/1.9 MB 398.7 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.8/1.9 MB 398.7 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.8/1.9 MB 398.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.9/1.9 MB 393.3 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.9/1.9 MB 397.0 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.9/1.9 MB 397.8 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 0.9/1.9 MB 397.7 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 0.9/1.9 MB 397.7 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 1.0/1.9 MB 397.6 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 1.0/1.9 MB 398.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 1.0/1.9 MB 397.3 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.0/1.9 MB 398.2 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.0/1.9 MB 398.9 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.1/1.9 MB 401.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 398.7 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 399.4 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 399.2 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 399.8 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.2/1.9 MB 398.9 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.2/1.9 MB 399.6 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.2/1.9 MB 396.8 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.2/1.9 MB 395.3 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.2/1.9 MB 395.9 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.2/1.9 MB 395.2 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.3/1.9 MB 395.8 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/1.9 MB 393.2 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/1.9 MB 395.8 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/1.9 MB 396.4 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.3/1.9 MB 395.7 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.4/1.9 MB 395.7 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.4/1.9 MB 396.3 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.4/1.9 MB 396.8 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.4/1.9 MB 394.4 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 396.7 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 396.1 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.5/1.9 MB 396.7 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.5/1.9 MB 396.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.5/1.9 MB 397.1 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.5/1.9 MB 396.5 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.5/1.9 MB 397.0 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.6/1.9 MB 397.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/1.9 MB 395.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/1.9 MB 394.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/1.9 MB 394.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/1.9 MB 390.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 388.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 388.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 388.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 388.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 388.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 388.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 388.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/1.9 MB 380.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 380.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 378.4 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 378.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 376.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 372.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 372.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 373.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 370.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 370.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 370.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 370.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/1.9 MB 369.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/1.9 MB 370.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/1.9 MB 368.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 369.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 368.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 367.1 kB/s eta 0:00:00\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-37.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f164d-7153-4eff-88e0-ecee03ae7df2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Insurance Risk & Claim & Fraudulent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75c2cc8-31b9-41ef-a290-8b43fd0cac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Synthetic dataset generated and saved as 'insurance_class_reg_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize faker\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "num_records = 10000\n",
    "\n",
    "# Helper functions\n",
    "def generate_policy_id(i):\n",
    "    return f\"POL{100000 + i}\"\n",
    "\n",
    "def generate_customer_id(i):\n",
    "    return f\"CUST{100000 + i}\"\n",
    "\n",
    "def generate_gender():\n",
    "    return random.choices(['Male', 'Female', 'Other'], weights=[0.48, 0.48, 0.04])[0]\n",
    "\n",
    "def generate_policy_type():\n",
    "    return random.choice(['Health', 'Auto', 'Life', 'Property'])\n",
    "\n",
    "def add_noise(val, scale=0.05):\n",
    "    noise = val * np.random.normal(0, scale)\n",
    "    return max(val + noise, 0)  # Ensure value is non-negative, = 0 is accepted\n",
    " \n",
    "# Generate data\n",
    "data = []\n",
    "\n",
    "for i in range(num_records):\n",
    "    customer_id = generate_customer_id(i)\n",
    "    policy_id = generate_policy_id(i)\n",
    "    age = int(np.clip(np.random.normal(40, 12), 18, 80))\n",
    "    gender = generate_gender()\n",
    "    policy_type = generate_policy_type()\n",
    "    income = max(round(np.random.normal(60000, 15000), 2), 5000)\n",
    "    asset_age = int(np.clip(np.random.normal(5, 2), 0, 20))  # Vehicle or Property age\n",
    "    claim_history = np.random.poisson(1.2)\n",
    "    fraudulent_claim = np.random.choice([0, 1], p=[0.8, 0.2])  # Imbalance\n",
    "    premium_amount = round(add_noise(1000 + claim_history * 100 + (age/2)), 2)\n",
    "    claim_amount = round(add_noise(premium_amount * np.random.uniform(0.5, 3.0)), 2)\n",
    "    location = random.choice(['New York', 'Chicago', 'Los Angeles', 'Houston', 'Dallas'])\n",
    "    policy_upgrade = np.random.choice([0, 1, 2], p=[0.5, 0.4, 0.1])\n",
    "    \n",
    "    #if fraudulent_claim == 1 or claim_history > 2 or claim_amount > 2500:\n",
    "     #   risk_score = np.random.choice(['Medium', 'High'], p=[0.3, 0.7])\n",
    "    #else:\n",
    "     #   risk_score = np.random.choice(['Low', 'Medium'], p=[0.6, 0.4])\n",
    "    \n",
    "    risk_index = (\n",
    "    2 * fraudulent_claim + \n",
    "    1.5 * (claim_history > 2) + \n",
    "    1.5 * (claim_amount > 2500) +\n",
    "    1 * (income < 30000) + \n",
    "    1 * (policy_type in ['Auto', 'Property']))\n",
    "\n",
    "    if risk_index >= 4:\n",
    "        risk_score = 'High'\n",
    "    elif risk_index >= 2:\n",
    "        risk_score = 'Medium'\n",
    "    else:\n",
    "        risk_score = 'Low'\n",
    "\n",
    "    \n",
    "    row = [\n",
    "        customer_id,\n",
    "        policy_id,\n",
    "        age,\n",
    "        gender,\n",
    "        policy_type,\n",
    "        income,\n",
    "        asset_age,\n",
    "        claim_history,\n",
    "        fraudulent_claim,\n",
    "        premium_amount,\n",
    "        claim_amount,\n",
    "        risk_score,\n",
    "        location,\n",
    "        policy_upgrade\n",
    "    ]\n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\n",
    "    'Customer_ID',\n",
    "    'Policy_ID',  \n",
    "    'Customer_Age',\n",
    "    'Gender',\n",
    "    'Policy_Type',\n",
    "    'Monthly_Income',\n",
    "    'Vehicle_or_Property_Age',\n",
    "    'Claim_History',\n",
    "    'Fraudulent_Claim',\n",
    "    'Premium_Amount',\n",
    "    'Claim_Amount',\n",
    "    'Risk_Score',\n",
    "    'Location',\n",
    "    'Policy_Upgrade'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"insurance_class_reg_data.csv\", index=False)\n",
    "print(\"✅ Synthetic dataset generated and saved as 'insurance_class_reg_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915de0b0-ff0b-4c18-b2da-c059eebc6cd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fraudulent Claim Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f412f96e-5d51-4ddb-abad-bece4ea4823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 6000 claim records using Faker.\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of claims\n",
    "num_claims = 6000\n",
    "\n",
    "# Generate unique Claim IDs\n",
    "claim_ids = [f\"CLM{100000 + i}\" for i in range(num_claims)]\n",
    "\n",
    "# Generate random Policyholder IDs from CUST100000 to CUST109999\n",
    "policyholder_ids = [f\"CUST{random.randint(100000, 109999)}\" for _ in range(num_claims)]\n",
    "\n",
    "# Convert date strings to datetime.date objects\n",
    "start_date = date(2023, 1, 1)\n",
    "end_date = date(2024, 12, 31)\n",
    "\n",
    "# Generate realistic Claim Dates (random between Jan 1, 2023 and Dec 31 2024)\n",
    "claim_dates = [fake.date_between(start_date=start_date, end_date=end_date) for _ in range(num_claims)]\n",
    "\n",
    "# Build DataFrame\n",
    "df_claims = pd.DataFrame({\n",
    "    'Claim_ID': claim_ids,\n",
    "    'Policyholder_ID': policyholder_ids,\n",
    "    'Claim_Date': claim_dates\n",
    "})\n",
    "\n",
    "# Save to CSV (optional)\n",
    "df_claims.to_csv(\"claim_base_data.csv\", index=False)\n",
    "print(\"✅ Generated 6000 claim records using Faker.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef897a77-1ec4-49b9-a7fc-6bbb9b859856",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Customer Feedback & Sentiment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4749ad-3aea-471e-b81d-c2f16e3fbc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Synthetic dataset with 5000 rows created and saved as 'insurance_reviews.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "# Define possible values for categorical columns\n",
    "sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "ratings_map = {\n",
    "    'Positive': [4, 5],\n",
    "    'Negative': [1, 2],\n",
    "    'Neutral': [3]\n",
    "}\n",
    "service_types = ['Claim', 'Policy Purchase', 'Customer Support']\n",
    "\n",
    "# Some sample review texts to simulate real data\n",
    "sample_reviews = {\n",
    "    'Positive': [\n",
    "        \"The claim process was quick and smooth.\",\n",
    "        \"Great customer support. Very satisfied.\",\n",
    "        \"Easy to buy a policy online. Loved it!\",\n",
    "        \"Fast and helpful service!\",\n",
    "        \"Everything went better than expected.\"\n",
    "    ],\n",
    "    'Negative': [\n",
    "        \"My claim was denied unfairly. Very disappointed.\",\n",
    "        \"Customer service was rude and unhelpful.\",\n",
    "        \"The policy terms were not clearly explained.\",\n",
    "        \"Had to wait weeks for a response.\",\n",
    "        \"Very bad experience with the support team.\"\n",
    "    ],\n",
    "    'Neutral': [\n",
    "        \"Bought the policy without any issues.\",\n",
    "        \"Service was okay, nothing special.\",\n",
    "        \"Average experience, neither good nor bad.\",\n",
    "        \"It was a typical insurance interaction.\",\n",
    "        \"Support was fine, not exceptional.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate synthetic data\n",
    "data = []\n",
    "for i in range(2000):\n",
    "    review_id = f\"R{str(i+1).zfill(5)}\"\n",
    "    customer_id = f\"CUST{str(random.randint(100000, 109999))}\"\n",
    "    sentiment = random.choice(sentiments)\n",
    "    review_text = random.choice(sample_reviews[sentiment])\n",
    "    rating = random.choice(ratings_map[sentiment])\n",
    "    service_type = random.choice(service_types)\n",
    "\n",
    "    data.append([review_id, customer_id, review_text, sentiment, rating, service_type])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    'Review_ID', 'Customer_ID', 'Review_Text', 'Sentiment_Label', 'Rating', 'Service_Type'\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('insurance_reviews.csv', index=False)\n",
    "\n",
    "print(\"✅ Synthetic dataset with 5000 rows created and saved as 'insurance_reviews.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0cf14f-22e5-4343-8fa2-db4cf9635f19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Insurance Multilingual Policy Document Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81df9e49-5c1c-43a4-aab3-df48d6fef2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with shape: (17482, 18)\n",
      "Saved generated policy texts to: policy_texts_en.csv (15998, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# reproducibility\n",
    "RND_SEED = 42\n",
    "random.seed(RND_SEED)\n",
    "np.random.seed(RND_SEED)\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "# Path to your dataset\n",
    "INPUT_CSV = \"C://Users//Admin//Documents//Guvi//MDTM38//project//insurance//data//insurance_af_cus_seg.csv\"  # <- replace with your file path\n",
    "OUTPUT_CSV = \"policy_texts_en.csv\"\n",
    "# ----------------------------\n",
    "\n",
    "def safe_num(x, fallback=5000):\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return fallback\n",
    "        return float(x)\n",
    "    except:\n",
    "        return fallback\n",
    "\n",
    "def norm_policy_type(s):\n",
    "    if pd.isna(s): \n",
    "        return \"General\"\n",
    "    s = str(s).strip().lower()\n",
    "    if \"auto\" in s or \"vehicle\" in s or \"car\" in s:\n",
    "        return \"Auto\"\n",
    "    if \"health\" in s or \"medical\" in s:\n",
    "        return \"Health\"\n",
    "    if \"life\" in s:\n",
    "        return \"Life\"\n",
    "    if \"property\" in s or \"home\" in s or \"house\" in s:\n",
    "        return \"Property\"\n",
    "    return s.title()\n",
    "\n",
    "def generate_policy_text(row, verbosity=3):\n",
    "    pid = row.get(\"policy_id\", \"UNKNOWN\")\n",
    "    ptype = norm_policy_type(row.get(\"policy_type\", \"General\"))\n",
    "    premium = safe_num(row.get(\"premium_amount\", np.nan), fallback=5000)\n",
    "    monthly_income = safe_num(row.get(\"monthly_income\", np.nan), fallback=4000)\n",
    "    vehicle_age = row.get(\"vehicle_or_property_age\", None)\n",
    "    claim_hist = int(row.get(\"claim_history\", 0) if not pd.isna(row.get(\"claim_history\", 0)) else 0)\n",
    "    ref = premium if premium > 0 else max(1, monthly_income * 12.0)\n",
    "    lead_phrases = [\n",
    "        \"This policy provides comprehensive coverage for\",\n",
    "        \"This insurance covers\",\n",
    "        \"Designed to protect against\"\n",
    "    ]\n",
    "    lead = random.choice(lead_phrases)\n",
    "    if ptype == \"Auto\":\n",
    "        cov_limit = max(50000, int(ref * random.uniform(10, 40)))\n",
    "        theft_limit = int(cov_limit * random.uniform(0.25, 0.6))\n",
    "        personal_acc_limit = int(cov_limit * random.uniform(0.05, 0.2))\n",
    "        deductible = int(max(250, ref * random.uniform(0.01, 0.08)))\n",
    "        parts = [\n",
    "            f\"{lead} vehicle damage due to accidents, collision, theft, fire, and natural hazards.\",\n",
    "            f\"Coverage includes comprehensive repair costs and third-party liability up to ${cov_limit:,}.\",\n",
    "            f\"Theft & vandalism cover limit: ${theft_limit:,}.\",\n",
    "            f\"Personal accident cover for driver and passengers up to ${personal_acc_limit:,}.\",\n",
    "            f\"Standard deductible: ${deductible:,} per claim. Optional add-ons: roadside assistance, zero-depreciation, and engine protection.\",\n",
    "            \"Exclusions: intentional damage, driving under influence, and racing-related incidents.\",\n",
    "        ]\n",
    "    elif ptype == \"Health\":\n",
    "        cov_limit = max(100000, int(ref * random.uniform(8, 80)))\n",
    "        in_patient_limit = int(cov_limit * random.uniform(0.6, 0.95))\n",
    "        daycare_limit = int(cov_limit * random.uniform(0.05, 0.2))\n",
    "        deductible = int(max(0, ref * random.uniform(0.01, 0.03)))\n",
    "        parts = [\n",
    "            f\"{lead} medical expenses including hospitalization, surgery, and day-care procedures.\",\n",
    "            f\"Sum insured: up to ${cov_limit:,}; in-patient treatment covered up to ${in_patient_limit:,}.\",\n",
    "            f\"Day-care & outpatient limits: up to ${daycare_limit:,} depending on treatment type.\",\n",
    "            \"Optional riders: maternity cover, critical illness rider, and pre/post-hospitalization benefits.\",\n",
    "        ]\n",
    "    elif ptype == \"Life\":\n",
    "        death_benefit = max(100000, int(ref * random.uniform(20, 200)))\n",
    "        critical_illness = int(death_benefit * random.uniform(0.2, 0.6))\n",
    "        parts = [\n",
    "            f\"{lead} the insured's family against loss of life, providing a death benefit of up to ${death_benefit:,}.\",\n",
    "            f\"Optional critical illness benefit: lump-sum up to ${critical_illness:,} for specified conditions.\",\n",
    "        ]\n",
    "    elif ptype == \"Property\":\n",
    "        cov_limit = max(100000, int(ref * random.uniform(10, 120)))\n",
    "        contents_limit = int(cov_limit * random.uniform(0.2, 0.6))\n",
    "        structural_limit = int(cov_limit * random.uniform(0.6, 0.95))\n",
    "        deductible = int(max(500, ref * random.uniform(0.01, 0.05)))\n",
    "        parts = [\n",
    "            f\"{lead} loss or damage to property from fire, lightning, explosion, burglary, and certain natural disasters.\",\n",
    "            f\"Structural damage cover: up to ${structural_limit:,}; contents & valuables covered up to ${contents_limit:,}.\",\n",
    "        ]\n",
    "    else:\n",
    "        cov_limit = int(ref * random.uniform(5, 40))\n",
    "        parts = [\n",
    "            f\"{lead} risks relevant to {ptype} policies, with cover limits tailored to the insured's profile.\",\n",
    "            f\"Indicative cover limit: ${cov_limit:,}.\",\n",
    "        ]\n",
    "    if verbosity <= 1:\n",
    "        text = \" \".join(parts[:2])\n",
    "    else:\n",
    "        text = \" \".join(parts)\n",
    "    return text.strip()\n",
    "\n",
    "if not Path(INPUT_CSV).exists():\n",
    "    print(f\"Input file {INPUT_CSV} not found.\")\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    print(\"Loaded dataset with shape:\", df.shape)\n",
    "    want_cols = [\"policy_id\", \"policy_type\", \"premium_amount\", \"monthly_income\", \"vehicle_or_property_age\", \"claim_history\", \"num_active_policies\"]\n",
    "    present = [c for c in want_cols if c in df.columns]\n",
    "    policies = df.drop_duplicates(\"policy_id\")[present].reset_index(drop=True)\n",
    "    policies[\"Policy_Text_EN\"] = policies.apply(lambda r: generate_policy_text(r, verbosity=3), axis=1)\n",
    "    policies.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(\"Saved generated policy texts to:\", OUTPUT_CSV, policies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00975659-ae97-4bbb-8c36-6a634bd1cd4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Customer Segmentation Dataset (Unsupervised Learning - Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3efaf236-18b8-4e41-92fa-b4a4a727105a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extra policy records generated: 5998\n",
      "✅ Generated 4000 Customer -  Extra policy records using Faker.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Step 1: Generate 4000 unique customer_ids\n",
    "customer_ids = [f\"CUST{random.randint(100000, 109999)}\" for _ in range(4000)]\n",
    "\n",
    "# Step 2: Generate extra policies for each (1 or 2 per customer)\n",
    "extra_data = []\n",
    "policy_counter = 200000  # Start from a high policy_id to avoid clashes\n",
    "\n",
    "for cust_id in customer_ids:\n",
    "    num_policies = random.choice([1, 2])\n",
    "    \n",
    "    for _ in range(num_policies):\n",
    "        policy_id = f\"POL{policy_counter}\"\n",
    "        policy_counter += 1\n",
    "        \n",
    "        policy_type = random.choice(['Health', 'Auto', 'Life', 'Property'])\n",
    "        asset_age = int(np.clip(np.random.normal(5, 2), 0, 20))\n",
    "        claim_history = np.random.poisson(1.2)\n",
    "        fraudulent_claim = np.random.choice([0, 1], p=[0.8, 0.2])\n",
    "        premium_amount = round(1000 + claim_history * 100 + (random.randint(25, 60) / 2), 2)\n",
    "\n",
    "        extra_data.append({\n",
    "            'customer_id': cust_id,\n",
    "            'policy_id': policy_id,\n",
    "            'policy_type': policy_type,\n",
    "            'vehicle_or_property_age': asset_age,\n",
    "            'claim_history': claim_history,\n",
    "            'fraudulent_claim': fraudulent_claim,\n",
    "            'premium_amount': premium_amount\n",
    "        })\n",
    "\n",
    "# Step 3: Create a new DataFrame\n",
    "df_extra_policies = pd.DataFrame(extra_data)\n",
    "\n",
    "# Preview\n",
    "print(\"✅ Extra policy records generated:\", df_extra_policies.shape[0])\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df_extra_policies.to_csv(\"extra_policies.csv\", index=False)\n",
    "print(\"✅ Generated 4000 Customer -  Extra policy records using Faker.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
